{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f5c463-8e10-4252-b3da-d557e5fa6289",
   "metadata": {},
   "source": [
    "# Document Classfication\n",
    "<hr>\n",
    "\n",
    "* 문서가 주어졌을 때 해당 문서의 범주를 분류하는 과제\n",
    "\n",
    "Ex) input : 뉴스, output : 정치, 경제, 연예 등 (범주 맞춤)\n",
    "<br><br>\n",
    "\n",
    "## NSMC Sentiment Analysis\n",
    "<hr>\n",
    "\n",
    "1. 입력 문장에 CLS, SEP 토큰을 붙인다.(CLS, SEP -> 문장 시작과 끝을 알리는 스페셜 토큰)\n",
    "2. BERT 모델에 입력 ->  문장 수준의 벡터(pooler_output)을 뽑느다.\n",
    "3. 벡터에 작은 추가 모듈을 통해 모델의 전체 출력이 [긍정확률, 부정확률]로 맞춘다.\n",
    "<br>\n",
    "\n",
    "## TASK Module\n",
    "<hr>\n",
    "<b>pooler_output</b> 벡터 뒤에 붙는 추가 모듈의 구조는 다음 그림과 같다.\n",
    "우선 <b>pooler_output</b>  벡터에 드롭아웃을 적용 \n",
    "<br>드롭아웃을 적용한다는 의미는 그림에서 입력 벡터의 768개 각 요솟값 가운데 일부를 랜덤으로 0으로 바꿔 이후 계산에 포함하지 않도록 한다.\n",
    "<br>\n",
    "<img src=\"task.png\">\n",
    "출처 : ratsgo's NLPBOOK\n",
    "<br>\n",
    "\n",
    "그다음 가중치 행렬을 곱해 pooler_output을 분류해야 할 범주 수만큼의 차원을 갖는 벡터로 변환.\n",
    "<br>만일 pooler_output 벡터가 768차원이고 분류 대상 범주 수가 2개(긍정, 부정)라면 가중치 행렬 크기는 (768,2) \n",
    "<br>여기에 소프트맥스 함수를 취하면 모델의 최종 출력\n",
    "<br>\n",
    "이렇게 만든 모델의 최종 출력과 정답 레이블을 비교해 모델 출력이 정답 레이블과 최대한 같아지도록 태스크 모듈과 BERT 레이어를 포함한 모델 전체를 업데이트<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff163384-b68d-4d95-aa37-9c4d316c894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from Korpora import Korpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa069815-f8f6-44e9-9e14-333ca8b3d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 지원됨. GPU 사용 가능!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# MPS 지원 여부 확인\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # MPS 설정\n",
    "    print(\"MPS 지원됨. GPU 사용 가능!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU로 설정\n",
    "    print(\"MPS 사용 불가. CPU 사용 중!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9115a40-ccd6-400e-ac17-84cb469f9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 8.30MB/s]                     \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 7.25MB/s]                      \n"
     ]
    }
   ],
   "source": [
    "# NSMC 데이터 다운로드\n",
    "Korpora.fetch(\"nsmc\", force_download=True)\n",
    "\n",
    "# 데이터 경로 설정\n",
    "train_data_path = \"../data/nsmc/ratings_train.txt\"\n",
    "test_data_path = \"../data/nsmc/ratings_test.txt\"\n",
    "\n",
    "# 데이터셋 로드 및 변환\n",
    "def load_nsmc_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[1:]  # 첫 줄(헤더) 제거\n",
    "    texts, labels = [], []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        text, label = parts[1], int(parts[2])  # 리뷰 내용과 감성 라벨 (0: 부정, 1: 긍정)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return {\"text\": texts, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5260d68-8184-4d4d-8f02-cbf0dea8c564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8d08f40dc7414588fe5b77b6fb12e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd1e652b260471081d1e515aba9dbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(load_nsmc_data(train_data_path))\n",
    "test_dataset = Dataset.from_dict(load_nsmc_data(test_data_path))\n",
    "\n",
    "\n",
    "# 토크나이징\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4639b9f8-0144-4114-b517-52d2f3124fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 모델 설정\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# 평가 메트릭 수정 (evaluate 라이브러리 사용)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=16,  # MPS에서는 batch_size=8~16 추천\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Mac에서는 0~2가 적절함\n",
    "    pin_memory=False,  # MPS에서는 필요 없음\n",
    ")\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,  # MPS에서는 8~16 추천\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=False,  # MPS에서는 반드시 False\n",
    "    bf16=False,  # bfloat16도 지원 안 됨\n",
    "    gradient_accumulation_steps=2,  # 작은 배치 크기 해결책\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536652e1-d8c5-4ece-8760-3963b0c32887",
   "metadata": {},
   "source": [
    "\t•\t학습 관련 설정을 정의하는 부분\n",
    "\t•\toutput_dir=\"./results\" → 모델 가중치 저장 경로\n",
    "\t•\tevaluation_strategy=\"epoch\" → 매 epoch 마다 평가\n",
    "\t•\tlearning_rate=5e-5 → BERT 모델의 기본 학습률(learning rate)\n",
    "\t•\tper_device_train_batch_size=32 → GPU 사용 가능하면 배치 크기 32, 아니면 4\n",
    "\t•\tnum_train_epochs=3 → 총 3 epoch 학습\n",
    "\t•\tsave_strategy=\"epoch\" → 매 epoch마다 모델 가중치 저장\n",
    "\t•\tlogging_dir=\"./logs\" → 로그 저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74708f4-b052-48d5-96bd-802b1fe02b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/dt43ws7s4_z4rrsjwvzmwz740000gn/T/ipykernel_31211/169967503.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='14061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   68/14061 03:45 < 13:18:18, 0.29 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/trainer.py:2550\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m-> 2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bd1cf-2b85-48f9-9661-f4729bdf0c99",
   "metadata": {},
   "source": [
    "\t•\tTrainer를 사용해 모델 학습을 진행하는 코드\n",
    "\t•\tmodel=model → 사용할 BERT 모델\n",
    "\t•\targs=training_args → 학습 설정 (TrainingArguments)\n",
    "\t•\ttrain_dataset=tokenized_train → 훈련 데이터셋\n",
    "\t•\teval_dataset=tokenized_test → 검증 데이터셋\n",
    "\t•\ttokenizer=tokenizer → BERT 토크나이저\n",
    "\t•\tcompute_metrics=compute_metrics → 평가 메트릭 (정확도)\n",
    "\t•\t마지막으로 trainer.train()을 실행하면 모델 파인튜닝이 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1630c4-9cfc-4105-9002-556b0ab85a34",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fb806-f39a-4127-a06d-5b05c0fd4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-4 모델 환경 설정\n",
    "import torch\n",
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",\n",
    "    downstream_corpus_name=\"nsmc\",\n",
    "    downstream_model_dir=\"/gdrive/My drive/nlpbook/checkpoint-doccls\",\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate=5e-5,\n",
    "    max_seq_length=128,\n",
    "    epochs=3,\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8,\n",
    "    seed=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53580252-c0c2-4f11-bcd7-75d826a6c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-7 말뭉치 내려받기\n",
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453c20b-e991-44d4-ac9e-1a6404f87e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#코드 4-8 토크나이저 준비\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d44c56-97e8-4253-bbb9-5e7dc1a9a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-9 학습 데이터셋 구축\n",
    "from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset\n",
    "corpus = NsmcCorpus()\n",
    "train_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c543d-e1a7-4852-b9a2-77ad16e307e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-10 학습 데이터 로더 구축\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33f7f4-8c1f-411a-a5d1-bf0f8fa6da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-11 평가용 데이터 로더 구축\n",
    "from torch.utils.data import SequentialSampler\n",
    "val_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"test\",\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97725b4-7f73-417e-8f92-dd3984886a23",
   "metadata": {},
   "source": [
    "## 모델 불러오기\n",
    "<hr>\n",
    "이제 모델을 초기화하는 다음 코드를 실행. \n",
    "코드 4-4에서 <b>pretrained_model_name을 beomi/kcbert-base</b>로 지정했으므로 프리트레인을 마친 <b>BERT</b>로 <b>kcbert-base</b>를 사용합니다. \n",
    "<br>모델을 초기화하는 코드에서 <b>BertForSequenceClassification</b>은 프리트레인을 마친 BERT 모델 위에 <4-1>절에서 설명한 문서 분류용 태스크 모듈이 덧붙여진 형태의 모델 클래스\n",
    "<br>이 클래스는 허깅페이스에서 제공하는 transformers 라이브러리에 포함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c9d48-a5f7-43e5-992c-a687c3054779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-12 모델 초기화\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=corpus.num_labels,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308ae5e-8c0b-40d8-853e-b994647757d2",
   "metadata": {},
   "source": [
    "허깅페이스 모델 허브에 등록된 모델이라면 별다른 코드 수정 없이 <b>kcbert-base</b> 이외에 다른 언어 모델을 사용할 수 있다.<br> \n",
    "예를 들어 bert-base-uncased 모델은 구글이 공개한 다국어 BERT 모델\n",
    "<br>코드 4-4에서 pretrained_model_name에 이 모델명을 입력하면 해당 모델을 쓸 수 있다. \n",
    "<br>허깅페이스에 등록된 모델 목록은 huggingface.co/models에서 확인할 수 있다.\n",
    "<br>아울러 코드 4-8, 4-12에는 똑같은 모델 이름을 입력해야 한다.\n",
    "\n",
    "## 모델 학습시키기\n",
    "<hr>\n",
    "파이토치 라이트닝(pytorch lighting)이 제공하는 LightingModule 클래스를 상속받아 태스크(task)를 정의. \n",
    "<br>태스크에는 다음 그림처럼 모델과 옵티마이저, 학습 과정 등이 정의돼 있다.\n",
    "\n",
    "<그림 4-6 TASK의 역할>\n",
    "<img src=\"task2.jpeg\">\n",
    "출처 : ratsgo's NLPBOOK<br>\n",
    "다음 코드를 실행하면 문서 분류용 태스크를 정의할 수 있습니다. \n",
    "<br>코드 4-4에서 만든 학습 설정(args)과 코드 4-12에서 준비한 모델(model)을 ClassificationTask에 주입합니다. \n",
    "<br><b>ClassificationTask</b>에는 옵티마이저(optimizer), 러닝 레이트 스케줄러(learning rate scheduler)가 정의돼 있는데, 옵티마이저로는 아담(Adam), 러닝 레이트 스케줄러로는 ExponentialLR을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b227e-403f-4846-9244-a45decb2aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 4-13 TASK 정의\n",
    "from ratsnlp.nlpbook.classification import ClassificationTask\n",
    "task = ClassificationTask(model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d607fc4-8c11-4217-ac37-685aca8dac1b",
   "metadata": {},
   "source": [
    "모델 학습 과정은 눈을 가린 상태에서 산등성이를 한 걸음씩 내려가는 과정에 비유 가능. \n",
    "<br> ExponentialLR은 현재 에포크의 러닝 레이트를 '이전 에포크의 러닝 레이트  gamma'로 스케줄.\n",
    "<br>\n",
    "ratsgo.github.io/nlpbook/docs/doc_cls/detail\n",
    "<br>\n",
    "## Inference\n",
    "* 학습을 마친 모델로 실제 과제를 수행하는 행위나 그 과정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4220f-dc91-4aa3-bb59-c9b79c01698f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
