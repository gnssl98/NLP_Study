{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f5c463-8e10-4252-b3da-d557e5fa6289",
   "metadata": {},
   "source": [
    "# Document Classfication\n",
    "<hr>\n",
    "\n",
    "* ë¬¸ì„œê°€ ì£¼ì–´ì¡Œì„ ë•Œ í•´ë‹¹ ë¬¸ì„œì˜ ë²”ì£¼ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê³¼ì œ\n",
    "\n",
    "Ex) input : ë‰´ìŠ¤, output : ì •ì¹˜, ê²½ì œ, ì—°ì˜ˆ ë“± (ë²”ì£¼ ë§ì¶¤)\n",
    "<br><br>\n",
    "\n",
    "## NSMC Sentiment Analysis\n",
    "<hr>\n",
    "\n",
    "1. ì…ë ¥ ë¬¸ì¥ì— CLS, SEP í† í°ì„ ë¶™ì¸ë‹¤.(CLS, SEP -> ë¬¸ì¥ ì‹œì‘ê³¼ ëì„ ì•Œë¦¬ëŠ” ìŠ¤í˜ì…œ í† í°)\n",
    "2. BERT ëª¨ë¸ì— ì…ë ¥ ->  ë¬¸ì¥ ìˆ˜ì¤€ì˜ ë²¡í„°(pooler_output)ì„ ë½‘ëŠë‹¤.\n",
    "3. ë²¡í„°ì— ì‘ì€ ì¶”ê°€ ëª¨ë“ˆì„ í†µí•´ ëª¨ë¸ì˜ ì „ì²´ ì¶œë ¥ì´ [ê¸ì •í™•ë¥ , ë¶€ì •í™•ë¥ ]ë¡œ ë§ì¶˜ë‹¤.\n",
    "<br>\n",
    "\n",
    "## TASK Module\n",
    "<hr>\n",
    "<b>pooler_output</b> ë²¡í„° ë’¤ì— ë¶™ëŠ” ì¶”ê°€ ëª¨ë“ˆì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ë‹¤.\n",
    "ìš°ì„  <b>pooler_output</b>  ë²¡í„°ì— ë“œë¡­ì•„ì›ƒì„ ì ìš© \n",
    "<br>ë“œë¡­ì•„ì›ƒì„ ì ìš©í•œë‹¤ëŠ” ì˜ë¯¸ëŠ” ê·¸ë¦¼ì—ì„œ ì…ë ¥ ë²¡í„°ì˜ 768ê°œ ê° ìš”ì†Ÿê°’ ê°€ìš´ë° ì¼ë¶€ë¥¼ ëœë¤ìœ¼ë¡œ 0ìœ¼ë¡œ ë°”ê¿” ì´í›„ ê³„ì‚°ì— í¬í•¨í•˜ì§€ ì•Šë„ë¡ í•œë‹¤.\n",
    "<br>\n",
    "<img src=\"task.png\">\n",
    "ì¶œì²˜ : ratsgo's NLPBOOK\n",
    "<br>\n",
    "\n",
    "ê·¸ë‹¤ìŒ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ pooler_outputì„ ë¶„ë¥˜í•´ì•¼ í•  ë²”ì£¼ ìˆ˜ë§Œí¼ì˜ ì°¨ì›ì„ ê°–ëŠ” ë²¡í„°ë¡œ ë³€í™˜.\n",
    "<br>ë§Œì¼ pooler_output ë²¡í„°ê°€ 768ì°¨ì›ì´ê³  ë¶„ë¥˜ ëŒ€ìƒ ë²”ì£¼ ìˆ˜ê°€ 2ê°œ(ê¸ì •, ë¶€ì •)ë¼ë©´ ê°€ì¤‘ì¹˜ í–‰ë ¬ í¬ê¸°ëŠ” (768,2) \n",
    "<br>ì—¬ê¸°ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì·¨í•˜ë©´ ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥\n",
    "<br>\n",
    "ì´ë ‡ê²Œ ë§Œë“  ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ê³¼ ì •ë‹µ ë ˆì´ë¸”ì„ ë¹„êµí•´ ëª¨ë¸ ì¶œë ¥ì´ ì •ë‹µ ë ˆì´ë¸”ê³¼ ìµœëŒ€í•œ ê°™ì•„ì§€ë„ë¡ íƒœìŠ¤í¬ ëª¨ë“ˆê³¼ BERT ë ˆì´ì–´ë¥¼ í¬í•¨í•œ ëª¨ë¸ ì „ì²´ë¥¼ ì—…ë°ì´íŠ¸<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff163384-b68d-4d95-aa37-9c4d316c894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from Korpora import Korpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa069815-f8f6-44e9-9e14-333ca8b3d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS ì§€ì›ë¨. GPU ì‚¬ìš© ê°€ëŠ¥!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# MPS ì§€ì› ì—¬ë¶€ í™•ì¸\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # MPS ì„¤ì •\n",
    "    print(\"MPS ì§€ì›ë¨. GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPUë¡œ ì„¤ì •\n",
    "    print(\"MPS ì‚¬ìš© ë¶ˆê°€. CPU ì‚¬ìš© ì¤‘!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9115a40-ccd6-400e-ac17-84cb469f9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 8.30MB/s]                     \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 7.25MB/s]                      \n"
     ]
    }
   ],
   "source": [
    "# NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "Korpora.fetch(\"nsmc\", force_download=True)\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "train_data_path = \"../data/nsmc/ratings_train.txt\"\n",
    "test_data_path = \"../data/nsmc/ratings_test.txt\"\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜\n",
    "def load_nsmc_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[1:]  # ì²« ì¤„(í—¤ë”) ì œê±°\n",
    "    texts, labels = [], []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        text, label = parts[1], int(parts[2])  # ë¦¬ë·° ë‚´ìš©ê³¼ ê°ì„± ë¼ë²¨ (0: ë¶€ì •, 1: ê¸ì •)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return {\"text\": texts, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5260d68-8184-4d4d-8f02-cbf0dea8c564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8d08f40dc7414588fe5b77b6fb12e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd1e652b260471081d1e515aba9dbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(load_nsmc_data(train_data_path))\n",
    "test_dataset = Dataset.from_dict(load_nsmc_data(test_data_path))\n",
    "\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4639b9f8-0144-4114-b517-52d2f3124fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­ ìˆ˜ì • (evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=16,  # MPSì—ì„œëŠ” batch_size=8~16 ì¶”ì²œ\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Macì—ì„œëŠ” 0~2ê°€ ì ì ˆí•¨\n",
    "    pin_memory=False,  # MPSì—ì„œëŠ” í•„ìš” ì—†ìŒ\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,  # MPSì—ì„œëŠ” 8~16 ì¶”ì²œ\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=False,  # MPSì—ì„œëŠ” ë°˜ë“œì‹œ False\n",
    "    bf16=False,  # bfloat16ë„ ì§€ì› ì•ˆ ë¨\n",
    "    gradient_accumulation_steps=2,  # ì‘ì€ ë°°ì¹˜ í¬ê¸° í•´ê²°ì±…\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536652e1-d8c5-4ece-8760-3963b0c32887",
   "metadata": {},
   "source": [
    "\tâ€¢\tí•™ìŠµ ê´€ë ¨ ì„¤ì •ì„ ì •ì˜í•˜ëŠ” ë¶€ë¶„\n",
    "\tâ€¢\toutput_dir=\"./results\" â†’ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ê²½ë¡œ\n",
    "\tâ€¢\tevaluation_strategy=\"epoch\" â†’ ë§¤ epoch ë§ˆë‹¤ í‰ê°€\n",
    "\tâ€¢\tlearning_rate=5e-5 â†’ BERT ëª¨ë¸ì˜ ê¸°ë³¸ í•™ìŠµë¥ (learning rate)\n",
    "\tâ€¢\tper_device_train_batch_size=32 â†’ GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ë°°ì¹˜ í¬ê¸° 32, ì•„ë‹ˆë©´ 4\n",
    "\tâ€¢\tnum_train_epochs=3 â†’ ì´ 3 epoch í•™ìŠµ\n",
    "\tâ€¢\tsave_strategy=\"epoch\" â†’ ë§¤ epochë§ˆë‹¤ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥\n",
    "\tâ€¢\tlogging_dir=\"./logs\" â†’ ë¡œê·¸ ì €ì¥ ê²½ë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74708f4-b052-48d5-96bd-802b1fe02b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/dt43ws7s4_z4rrsjwvzmwz740000gn/T/ipykernel_31211/169967503.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='14061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   68/14061 03:45 < 13:18:18, 0.29 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/trainer.py:2550\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m-> 2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bd1cf-2b85-48f9-9661-f4729bdf0c99",
   "metadata": {},
   "source": [
    "\tâ€¢\tTrainerë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ì½”ë“œ\n",
    "\tâ€¢\tmodel=model â†’ ì‚¬ìš©í•  BERT ëª¨ë¸\n",
    "\tâ€¢\targs=training_args â†’ í•™ìŠµ ì„¤ì • (TrainingArguments)\n",
    "\tâ€¢\ttrain_dataset=tokenized_train â†’ í›ˆë ¨ ë°ì´í„°ì…‹\n",
    "\tâ€¢\teval_dataset=tokenized_test â†’ ê²€ì¦ ë°ì´í„°ì…‹\n",
    "\tâ€¢\ttokenizer=tokenizer â†’ BERT í† í¬ë‚˜ì´ì €\n",
    "\tâ€¢\tcompute_metrics=compute_metrics â†’ í‰ê°€ ë©”íŠ¸ë¦­ (ì •í™•ë„)\n",
    "\tâ€¢\të§ˆì§€ë§‰ìœ¼ë¡œ trainer.train()ì„ ì‹¤í–‰í•˜ë©´ ëª¨ë¸ íŒŒì¸íŠœë‹ì´ ì§„í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1630c4-9cfc-4105-9002-556b0ab85a34",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fb806-f39a-4127-a06d-5b05c0fd4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-4 ëª¨ë¸ í™˜ê²½ ì„¤ì •\n",
    "import torch\n",
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",\n",
    "    downstream_corpus_name=\"nsmc\",\n",
    "    downstream_model_dir=\"/gdrive/My drive/nlpbook/checkpoint-doccls\",\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate=5e-5,\n",
    "    max_seq_length=128,\n",
    "    epochs=3,\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8,\n",
    "    seed=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53580252-c0c2-4f11-bcd7-75d826a6c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-7 ë§ë­‰ì¹˜ ë‚´ë ¤ë°›ê¸°\n",
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453c20b-e991-44d4-ac9e-1a6404f87e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ì½”ë“œ 4-8 í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d44c56-97e8-4253-bbb9-5e7dc1a9a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-9 í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset\n",
    "corpus = NsmcCorpus()\n",
    "train_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c543d-e1a7-4852-b9a2-77ad16e307e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-10 í•™ìŠµ ë°ì´í„° ë¡œë” êµ¬ì¶•\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33f7f4-8c1f-411a-a5d1-bf0f8fa6da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-11 í‰ê°€ìš© ë°ì´í„° ë¡œë” êµ¬ì¶•\n",
    "from torch.utils.data import SequentialSampler\n",
    "val_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"test\",\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97725b4-7f73-417e-8f92-dd3984886a23",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "<hr>\n",
    "ì´ì œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰. \n",
    "ì½”ë“œ 4-4ì—ì„œ <b>pretrained_model_nameì„ beomi/kcbert-base</b>ë¡œ ì§€ì •í–ˆìœ¼ë¯€ë¡œ í”„ë¦¬íŠ¸ë ˆì¸ì„ ë§ˆì¹œ <b>BERT</b>ë¡œ <b>kcbert-base</b>ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "<br>ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì½”ë“œì—ì„œ <b>BertForSequenceClassification</b>ì€ í”„ë¦¬íŠ¸ë ˆì¸ì„ ë§ˆì¹œ BERT ëª¨ë¸ ìœ„ì— <4-1>ì ˆì—ì„œ ì„¤ëª…í•œ ë¬¸ì„œ ë¶„ë¥˜ìš© íƒœìŠ¤í¬ ëª¨ë“ˆì´ ë§ë¶™ì—¬ì§„ í˜•íƒœì˜ ëª¨ë¸ í´ë˜ìŠ¤\n",
    "<br>ì´ í´ë˜ìŠ¤ëŠ” í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì— í¬í•¨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c9d48-a5f7-43e5-992c-a687c3054779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-12 ëª¨ë¸ ì´ˆê¸°í™”\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=corpus.num_labels,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308ae5e-8c0b-40d8-853e-b994647757d2",
   "metadata": {},
   "source": [
    "í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ í—ˆë¸Œì— ë“±ë¡ëœ ëª¨ë¸ì´ë¼ë©´ ë³„ë‹¤ë¥¸ ì½”ë“œ ìˆ˜ì • ì—†ì´ <b>kcbert-base</b> ì´ì™¸ì— ë‹¤ë¥¸ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.<br> \n",
    "ì˜ˆë¥¼ ë“¤ì–´ bert-base-uncased ëª¨ë¸ì€ êµ¬ê¸€ì´ ê³µê°œí•œ ë‹¤êµ­ì–´ BERT ëª¨ë¸\n",
    "<br>ì½”ë“œ 4-4ì—ì„œ pretrained_model_nameì— ì´ ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ëª¨ë¸ì„ ì“¸ ìˆ˜ ìˆë‹¤. \n",
    "<br>í—ˆê¹…í˜ì´ìŠ¤ì— ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ì€ huggingface.co/modelsì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
    "<br>ì•„ìš¸ëŸ¬ ì½”ë“œ 4-8, 4-12ì—ëŠ” ë˜‘ê°™ì€ ëª¨ë¸ ì´ë¦„ì„ ì…ë ¥í•´ì•¼ í•œë‹¤.\n",
    "\n",
    "## ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°\n",
    "<hr>\n",
    "íŒŒì´í† ì¹˜ ë¼ì´íŠ¸ë‹(pytorch lighting)ì´ ì œê³µí•˜ëŠ” LightingModule í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ íƒœìŠ¤í¬(task)ë¥¼ ì •ì˜. \n",
    "<br>íƒœìŠ¤í¬ì—ëŠ” ë‹¤ìŒ ê·¸ë¦¼ì²˜ëŸ¼ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €, í•™ìŠµ ê³¼ì • ë“±ì´ ì •ì˜ë¼ ìˆë‹¤.\n",
    "\n",
    "<ê·¸ë¦¼ 4-6 TASKì˜ ì—­í• >\n",
    "<img src=\"task2.jpeg\">\n",
    "ì¶œì²˜ : ratsgo's NLPBOOK<br>\n",
    "ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë¬¸ì„œ ë¶„ë¥˜ìš© íƒœìŠ¤í¬ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "<br>ì½”ë“œ 4-4ì—ì„œ ë§Œë“  í•™ìŠµ ì„¤ì •(args)ê³¼ ì½”ë“œ 4-12ì—ì„œ ì¤€ë¹„í•œ ëª¨ë¸(model)ì„ ClassificationTaskì— ì£¼ì…í•©ë‹ˆë‹¤. \n",
    "<br><b>ClassificationTask</b>ì—ëŠ” ì˜µí‹°ë§ˆì´ì €(optimizer), ëŸ¬ë‹ ë ˆì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)ê°€ ì •ì˜ë¼ ìˆëŠ”ë°, ì˜µí‹°ë§ˆì´ì €ë¡œëŠ” ì•„ë‹´(Adam), ëŸ¬ë‹ ë ˆì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œëŠ” ExponentialLRì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b227e-403f-4846-9244-a45decb2aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½”ë“œ 4-13 TASK ì •ì˜\n",
    "from ratsnlp.nlpbook.classification import ClassificationTask\n",
    "task = ClassificationTask(model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d607fc4-8c11-4217-ac37-685aca8dac1b",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í•™ìŠµ ê³¼ì •ì€ ëˆˆì„ ê°€ë¦° ìƒíƒœì—ì„œ ì‚°ë“±ì„±ì´ë¥¼ í•œ ê±¸ìŒì”© ë‚´ë ¤ê°€ëŠ” ê³¼ì •ì— ë¹„ìœ  ê°€ëŠ¥. \n",
    "<br> ExponentialLRì€ í˜„ì¬ ì—í¬í¬ì˜ ëŸ¬ë‹ ë ˆì´íŠ¸ë¥¼ 'ì´ì „ ì—í¬í¬ì˜ ëŸ¬ë‹ ë ˆì´íŠ¸  gamma'ë¡œ ìŠ¤ì¼€ì¤„.\n",
    "<br>\n",
    "ratsgo.github.io/nlpbook/docs/doc_cls/detail\n",
    "<br>\n",
    "## Inference\n",
    "* í•™ìŠµì„ ë§ˆì¹œ ëª¨ë¸ë¡œ ì‹¤ì œ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ëŠ” í–‰ìœ„ë‚˜ ê·¸ ê³¼ì •\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4220f-dc91-4aa3-bb59-c9b79c01698f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
