{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aaec9e",
   "metadata": {},
   "source": [
    "# Do it! BERTì™€ GPTë¡œ ë°°ìš°ëŠ” ìì—°ì–´ ì²˜ë¦¬\n",
    "### ì €ì ì´ê¸°ì°½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d7731",
   "metadata": {},
   "source": [
    "# ë°”ì´íŠ¸ í˜ì–´ ì¸ì½”ë”©(Byte Parir Encoding : BPE)\n",
    "* ë³¸ë˜ ì •ë³´ë¥¼ ì••ì¶•í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì œì•ˆ -> ìµœê·¼ NLP ëª¨ë¸ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” í† í°í™” ê¸°ë²•\n",
    "* ë°ì´í„°ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë¬´ìì—´ì„ ë³‘í•©í•´ì„œ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ëŠ” ê¸°ë²•\n",
    "\n",
    "> aaabdaabac<br>\n",
    "\n",
    "<br>\n",
    "ë°ì´í„°ì— ë“±ì¥í•œ ê¸€ì(a,b,c,d)ë¥¼ ì´ˆê¸° ì‚¬ì „ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, ì—°ì†ëœ ë‘ ê¸€ìë¥¼ í•œ ê¸€ìë¡œ ë³‘í•©<br>\n",
    "aa-> Zë¡œ ë³‘í•©<br>\n",
    "<br>\n",
    "\n",
    "> ZabdZabac\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "abê°€ ë‹¤ìŒìœ¼ë¡œ ë§ì´ ë‚˜ì™”ìœ¼ë¯€ë¡œ ab -> Yë¡œ ë³‘í•©<br>\n",
    "<br>\n",
    "\n",
    "> ZYdZYac\n",
    "\n",
    "<br>\n",
    "ZYê°€ ë‹¤ìŒìœ¼ë¡œ ë§ì´ ë‚˜ì™”ìœ¼ë¯€ë¡œ ZY -> Xë¡œ ë³‘í•©(ì´ë¯¸ ë³‘í•©ëœ ë¬¸ìì—´ë„ ë‹¤ì‹œ ë³‘í•©)<br>\n",
    "<br>\n",
    "\n",
    "> XdXac\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ì‚¬ì „\n",
    "(a,b,c,d) -> (a,b,c,d,Z,Y,X)<br><br>\n",
    "5ê°œ -> 7ê°œ<br>\n",
    "len(ë°ì´í„°) 11 -> 5<br>\n",
    "<br>\n",
    "* BPE ê¸°ë°˜ í† í°í™” ê¸°ë²•ì€corpusì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë¬¸ìì—´(Subword)ì„ í† í°ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ë•Œë¬¸ì— ë¶„ì„ ëŒ€ìƒ ì–¸ì–´ì— ëŒ€í•œ ì§€ì‹ì´ í•„ìš” X<br>\n",
    "<br>\n",
    "## ê³¼ì •\n",
    "1. ì–´íœ˜ ì§‘í•© êµ¬ì¶•: ìì£¼ ë“±ì¥í•˜ëŠ” ë¬¸ìì—´ ë³‘í•©, ì–´íœ˜ ì§‘í•©ì— ì¶”ê°€(ì›í•˜ëŠ” ì–´íœ˜ ì§‘í•© í¬ê¸°ê°€ ë  ë•Œ ê¹Œì§€ ë°˜ë³µ)\n",
    "2. í† í°í™” : í† í°í™” ëŒ€ìƒ ë¬¸ì¥ì˜ ê° ì–´ì ˆì—ì„œ ì–´íœ˜ ì§‘í•©ì— ìˆëŠ” ì„œë¸Œì›Œë“œê°€ í¬í•¨ë˜ì—ˆì„ ë•Œ í•­ìƒ ì„œë¸Œì›Œë“œë¥¼ ì–´ì ˆì—ì„œ ë¶„ë¦¬\n",
    "\n",
    "\n",
    "# BRE ì–´íœ˜ ì§‘í•© êµ¬ì¶•\n",
    "\n",
    "### Pre-Tokenize\n",
    "Corpusì˜ ëª¨ë“  ë¬¸ì¥ì„ ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…\n",
    "\n",
    "1. Pre-Tokenizeì„ ì ìš©í•˜ê³  ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ëª¨ë‘ ì„¸ì–´ì„œ ì´ˆê¸°ì˜ ì–´íœ˜ ì§‘í•© êµ¬ì„±\n",
    "b,g,h,n,p,s,u\n",
    "\n",
    "\n",
    "### Pre-Tokenize ê²°ê³¼\n",
    "\n",
    "| í† í°  | ë¹ˆë„ |\n",
    "|------|------|\n",
    "| hug  | 10   |\n",
    "| pug  | 5    |\n",
    "| pun  | 12   |\n",
    "| bun  | 4    |\n",
    "| hugs | 5    |\n",
    "\n",
    "### ì´ˆê¸° ì–´íœ˜ ì§‘í•©ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•œ ë¹ˆë„í‘œ\n",
    "\n",
    "| í† í°        | ë¹ˆë„ |\n",
    "|------------|------|\n",
    "| h, u, g   | 10   |\n",
    "| p, u, g   | 5    |\n",
    "| p, u, n   | 12   |\n",
    "| b, u, n   | 4    |\n",
    "| h, u, g, s | 5    |\n",
    "\n",
    "ë‹¤ì‹œ ë‘ìŒìœ¼ë¡œ ë¬¶ì–´ì„œ bigram ìƒì„± ë° ë³‘í•©\n",
    "\n",
    "| ë°”ì´ê·¸ë¨ ìŒ | ë¹ˆë„ |\n",
    "|------------|------|\n",
    "| b, u      | 4    |\n",
    "| g, s      | 5    |\n",
    "| h, u      | 15   |\n",
    "| p, u      | 17   |\n",
    "| u, g      | 20   |\n",
    "| u, n      | 16   |\n",
    "\n",
    "u,g ê°€ 20ê°œë¡œ ì œì¼ ë§ì•„ì„œ ë³‘í•©(u,g -> ug)\n",
    "\n",
    "> b,g,h,n,p,s,u,ug\n",
    "\n",
    "| ë°”ì´ê·¸ë¨ ìŒ | ë¹ˆë„ |\n",
    "|------------|------|\n",
    "| b, u      | 4    |\n",
    "| h, ug      | 15    |\n",
    "| p, u      | 12   |\n",
    "| p, ug      | 5   |\n",
    "| u, n      | 16   |\n",
    "| ug, s     | 5  |\n",
    "\n",
    "u,n ê°€ 16ê°œë¡œ ì œì¼ ë§ì•„ì„œ ë³‘í•© (u,n -> un)\n",
    "\n",
    "> b,g,h,n,p,s,u,ug,un\n",
    "\n",
    "| ë°”ì´ê·¸ë¨ ìŒ | ë¹ˆë„ |\n",
    "|------------|------|\n",
    "| b, un     | 4    |\n",
    "| h, ug     | 15   |\n",
    "| p, ug     | 5    |\n",
    "| p, un     | 12   |\n",
    "| ug, s     | 5    |\n",
    "\n",
    "## BPE ì–´íœ˜ ì§‘í•© êµ¬ì¶•\n",
    "\n",
    "> b, g, h, n, p, s, u, ug, un, hug\n",
    "\n",
    "## BPE ì–´íœ˜ ì§‘í•©ì€ ê³ ë¹ˆë„ ë°”ì´ê·¸ë¨ ìŒì„ ë³‘í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬ì¶•\n",
    "\n",
    "ì²˜ìŒ ë³‘í•©í•œ ëŒ€ìƒì€ u, g, ë‘ ë²ˆì§¸ëŠ” u, n, ë§ˆì§€ë§‰ì€ h, ugì˜€ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.<br>\n",
    "ì´ ë‚´ìš© ê·¸ëŒ€ë¡œ merges.txtë¡œ ì €ì¥<br>\n",
    "ë³‘í•© ìš°ì„ ìˆœìœ„(merges.txt)<br>\n",
    "u g<br>\n",
    "u n<br>\n",
    "h ug<br>\n",
    "\n",
    "\n",
    "# BPE í† í°í™”\n",
    "\n",
    "ì–´íœ˜ ì§‘í•©(vocab.json), ë³‘í•© ìš°ì„ ìˆœìœ„(merge.txt)ê°€ ìˆìœ¼ë©´ í† í°í™” ìˆ˜í–‰ ê°€ëŠ¥\n",
    "<br>\n",
    "ex) pug bug mug ë¼ëŠ” ë¬¸ì¥ì„ í† í°í™”\n",
    "\n",
    "> pug bug mug -> pug,bug,mug\n",
    "\n",
    "1. ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "\n",
    "> pug -> p,u,g\n",
    "\n",
    "2. merge.txt íŒŒì¼ì„ ì°¸ê³ í•´ ë³‘í•© ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬\n",
    "\n",
    "> p,u -> ìš°ì„  ìˆœìœ„ ì—†ìŒ\n",
    "<br> u,g -> 1ìˆœìœ„\n",
    "\n",
    "3. u,gì˜ ìš°ì„  ìˆœìœ„ê°€ ë†’ìœ¼ë¯€ë¡œ ë¨¼ì € í•©ì¹œë‹¤.\n",
    "\n",
    "> p,u,g -> p, ug\n",
    "\n",
    "4. merge.txt íŒŒì¼ì„ í•œë²ˆ ë” ì°¸ê³ í•´ ë³‘í•© ìš°ì„ ìˆ˜ìœ„ë¥¼ ë¶€ì—¬\n",
    "\n",
    "> p,ug -> ìš°ì„  ìˆœìœ„ ì—†ìŒ\n",
    "\n",
    "ë³‘í•©í•  ëŒ€ìƒì´ ì—†ì–´ì„œ stop <br>\n",
    "ì´ ìˆœì„œë¥¼ ë°˜ë³µ<br>\n",
    "&lt;unk&gt; -> ë¯¸ë“±ë¡ í† í°<br>\n",
    "ì—¬ê¸°ì„œ mì´ ì–´íœ˜ ì§‘í•©ì— ì—†ì–´ì„œ ë¯¸ë“±ë¡ í† í°ì´ ëœë‹¤.\n",
    "\n",
    "> pug bug mug -> p, ug, b, ug, &lt;unk&gt;, ug\n",
    "\n",
    "# WordPiece\n",
    "\n",
    "* Corpusì—ì„œ ìì£¼ ë“±ì¥í•œ ë¬¸ìì—´ì„ í† í°ìœ¼ë¡œ ì¸ì‹ -> BPEì™€ ë³¸ì§ˆì ìœ¼ë¡œ ìœ ì‚¬\n",
    "* ì–´íœ˜ ì§‘í•©ì„ êµ¬ì¶•í•  ë•Œ ë¬¸ìì—´ì„ ë³‘í•©í•˜ëŠ” ê¸°ì¤€ì´ ë‹¤ë¦„\n",
    "* ë¹ˆë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© x -> ë³‘í•©í–ˆì„ ë•Œ Corpusì˜ Likelihoodë¥¼ ê°€ì¥ ë†’ì´ëŠ” ìŒì„ ë³‘í•©\n",
    "\n",
    "<br>\n",
    "ë³‘í•© í›„ë³´ê°€ a,b ì¼ë•Œ, #a, #b, #abëŠ” ê°ê° a,b,abë¼ëŠ” ë¬¸ìì—´ì˜ ë¹ˆë„ìˆ˜, nì€ ì „ì²´ ê¸€ì ìˆ˜ë¥¼ ê°€ë¦¬í‚¨ë‹¤.\n",
    "<br>\n",
    "ë¶„ìëŠ” abê°€ ì—°ì´ì–´ ë“±ì¥í•  í™•ë¥ , ë¶„ëª¨ëŠ” a,bê°€ ê°ê° ë“±ì¥í•  í™•ë¥ ì˜ ê³±\n",
    "<br>\n",
    "\n",
    "$\\frac{\\left(\\frac{\\# ab}{n} \\right)}{\\left(\\frac{\\# a}{n} \\right)} \\times \\left(\\frac{\\# b}{n} \\right)$\n",
    "\n",
    "ì´ ìˆ˜ì‹ì˜ ê°’ì´ ì»¤ì§€ë ¤ë©´ aì™€ bê°€ ì„œë¡œ ë…ë¦½ì„ì„ ê°€ì •í–ˆì„ ë•Œë³´ë‹¤ ë‘˜ì´ ìì£¼ ë™ì‹œì— ë“±ì¥í•´ì•¼ í•œë‹¤.<br>\n",
    "ì›Œë“œí”¼ìŠ¤ì—ì„œëŠ” ë³‘í•© í›„ë³´ì— ì˜¤ë¥¸ ìŒì„ ë¯¸ë¦¬ ë³‘í•©í•´ ë³´ê³  ìƒëŠ” ê²ƒê³¼ ê°€ì¹˜ ë“±ì„ íŒë‹¨í•œ í›„ì— ë³‘í•©. (ë³‘í•© ëŒ€ìƒ ì „ì²´ í›„ë³´ë“¤ ê°€ìš´ë° ìœ„ì™€ ê°™ì´ ê³„ì‚°í•œ ê°’ì´ ê°€ì¥ ë†’ì€ ìŒì„ í•©ì¹œë‹¤.)\n",
    "<br>\n",
    "\n",
    "ì›Œë“œí”¼ìŠ¤ëŠ” ì–´íœ˜ ì§‘í•©(vocab.txt)ë§Œ ê°€ì§€ê³  í† í°í™”\n",
    "<br>\n",
    "ì›Œë“œí”¼ìŠ¤ì—ì„œëŠ” ë¶„ì„ ëŒ€ìƒ ì–´ì ˆì— ì–´íœ˜ ì§‘í•©ì— ìˆëŠ” ì„œë¸Œì›Œë“œê°€ í¬í™¤ë¼ ìˆì„ ë•Œ í•´ë‹¹ ì„œë¸Œì›Œë“œë¥¼ ì–´ì ˆì—ì„œ ë¶„ë¦¬<br>\n",
    "ë‹¨, ì´ëŸ¬í•œ ì„œë¸Œì›Œë“œ í›„ë³´ê°€ ì—¬ëŸ¿ ìˆì„ ê²½ìš° ê°€ì¥ ê¸´ ì„œë¸Œì›Œë“œë¥¼ ì„ íƒ.\n",
    "<br>\n",
    "ì´í›„ ì–´ì ˆì˜ ë‚˜ë¨¸ì§€ì—ì„œ ì–´íœ˜ ì§‘í•©ì— ìˆëŠ” ì„œë¸Œì›Œë“œë¥¼ ë‹¤ì‹œ ì°¾ê³ (ìµœì¥ ì¼ì¹˜ ê¸°ì¤€), ë˜ ë¶„ë¦¬\n",
    "<br>\n",
    "ë¶„ì„ ëŒ€ìƒ ë¬¸ìì—´ì—ì„œ ì„œë¸Œì›Œë“œ í›„ë³´ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ í•´ë‹¹ ë¬¸ìì—´ ì „ì²´ë¥¼ ë¯¸ë“±ë¡ ë‹¨ì–´ë¡œ ì·¨ê¸‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5d2f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
      "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
      "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
      "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 11.0MB/s]                     \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 9.88MB/s]                      \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "nsmc = Korpora.load(\"nsmc\", force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd183cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def write_lines(path, lines):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(f'{line}\\n')\n",
    "write_lines(\"./data/tokenizer/train.txt\", nsmc.train.get_all_texts())\n",
    "write_lines(\"./data/tokenizer/test.txt\", nsmc.test.get_all_texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfe07d3-2bd4-41e7-8a72-d512aeb4abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./bbpe\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10c693-e322-4776-84b3-c941c661af10",
   "metadata": {},
   "source": [
    "vocab.jsonê³¼ merges.txtê°€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80948ecd-162a-45fa-9362-76e4a8f4439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./bbpe/vocab.json', './bbpe/merges.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "bytebpe_tokenizer.train(\n",
    "    files=[\"./data/tokenizer/train.txt\",\"./data/tokenizer/test.txt\"],# corpusë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë„£ê¸°\n",
    "          vocab_size=10000, # ì–´íœ˜ ì§‘í•© í¬ê¸° ì¡°ì ˆ\n",
    "          special_tokens=[\"[PAD]\"] #íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    ")\n",
    "bytebpe_tokenizer.save_model(\"./bbpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6b6ca1-367f-42cc-aeb7-e718d78babb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./wordpiece/vocab.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "wordpiece_tokenizer.train(\n",
    "    files=[\"./data/tokenizer/train.txt\",\"./data/tokenizer/test.txt\"],\n",
    "    vocab_size=10000,\n",
    ")\n",
    "wordpiece_tokenizer.save_model(\"./wordpiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd90003b-e881-4a45-9071-4e99ff2dcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT í† í¬ë‚˜ì´ì € ì„ ì–¸\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"./bbpe\")\n",
    "tokenizer_gpt.pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e325feab-8539-49f2-b117-89bf13619945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•˜ê¸°\n",
    "sentences = [\n",
    "    \"ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\",\n",
    "    \"í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\",\n",
    "    \"ë³„ë£¨ ì˜€ë‹¤..\",\n",
    "]\n",
    "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2016f1-3179-41ca-ace9-01f2078c0d04",
   "metadata": {},
   "source": [
    "## tokenized_sentences\n",
    "* GPT2 í† í¬ë‚˜ì´ì €(tokenizer_gpt)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë³„ ë¬¸ì¥ì„ í† í°í™”\n",
    "* .tokenize(sentence): í•´ë‹¹ ë¬¸ì¥ì„ ê°œë³„ í† í° ë‹¨ìœ„ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4368b283-34c1-4c22-9c16-ce3650a21149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ã¬Ä·Ä¦', 'Ä Ã«Ä¯Ä¶Ã«Â¹Ä»', '..', 'Ä Ã¬Â§Ä¦Ã¬Â§Ä¾', 'Ä Ã¬Â§Ä¾Ã¬Â¦Ä¿Ã«Ä¤Äº', 'Ã«Ä¦Â¤Ã¬Ä¼Ä¶', 'Ä Ã«ÂªÂ©Ã¬Ä¨Ä®Ã«Â¦Â¬'],\n",
       " ['Ã­Ä¿Å‚',\n",
       "  '...',\n",
       "  'Ã­Ä±Â¬Ã¬Ä¬Â¤Ã­Ä¦Â°',\n",
       "  'Ã«Â³Â´ÃªÂ³Å‚',\n",
       "  'Ä Ã¬Â´ÄªÃ«Ä¶Â©',\n",
       "  'Ã¬ÄºÄ£Ã­Ä»Ä¶',\n",
       "  'Ã¬Â¤Ä¦',\n",
       "  '....',\n",
       "  'Ã¬ÄºÂ¤Ã«Â²Ä¦',\n",
       "  'Ã¬Ä¹Â°ÃªÂ¸Â°',\n",
       "  'Ã¬Â¡Â°Ã¬Â°Â¨',\n",
       "  'Ä ÃªÂ°Ä¢Ã«Â³Ä¯',\n",
       "  'Ã¬Â§Ä¢',\n",
       "  'Ä Ã¬Ä·Ä¬',\n",
       "  'ÃªÂµÂ¬Ã«Ä¤Äº'],\n",
       " ['Ã«Â³Ä¦Ã«Â£Â¨', 'Ä Ã¬Äº', 'Ä¢Ã«Ä­Â¤', '..']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f5b49fc-de78-46ba-9c23-4167073e80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT ëª¨ë¸ ì…ë ¥ ë§Œë“¤ê¸°\n",
    "batch_inputs = tokenizer_gpt(\n",
    "    sentences,\n",
    "    padding=\"max_length\",  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©\n",
    "    max_length=12,  # ë¬¸ì¥ì˜ í† í° ê¸°ì¤€ ìµœëŒ€ ê¸¸ì´\n",
    "    truncation=True,  # ë¬¸ì¥ ì˜ë¦¼ í—ˆìš© ì˜µì…˜\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2e167-5c91-4d12-8345-53db7c53d118",
   "metadata": {},
   "source": [
    "## batch_inputs\n",
    "ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ì¸ì½”ë”©\n",
    "* .tokenize()ì™€ ë‹¤ë¥´ê²Œ í† í°í™” + ìˆ«ì ë³€í™˜(input_ids) + íŒ¨ë”©(attention_mask) ì ìš©ë¨\n",
    "* max_length=12: ìµœëŒ€ 12ê°œ í† í°ê¹Œì§€ ìœ ì§€ (ì´ë³´ë‹¤ ê¸¸ë©´ ìë¦„)\n",
    "* padding=\"max_length\": ë¶€ì¡±í•œ ê¸¸ì´ëŠ” [PAD] í† í°ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬\n",
    "* truncation=True: ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ìë™ìœ¼ë¡œ ì˜ë¦¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15109eb4-6465-4962-b116-5392666a6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0], [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1162, 16, 3], [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e4208d3-48d3-43f6-af7e-f5a93b6f4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT í† í¬ë‚˜ì´ì € ì„ ì–¸\n",
    "from transformers import BertTokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\n",
    "    \"./wordpiece\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39fb816e-0b24-4d2e-802f-377d772ee8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•˜ê¸°\n",
    "sentences = [\n",
    "    \"ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\",\n",
    "    \"í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\",\n",
    "    \"ë³„ë£¨ ì˜€ë‹¤..\",\n",
    "]\n",
    "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d6debcc-058f-4a2b-999c-f76bb88fbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT ëª¨ë¸ ì…ë ¥ ë§Œë“¤ê¸°\n",
    "batch_inputs = tokenizer_bert(\n",
    "    sentences,\n",
    "    padding=\"max_length\",\n",
    "    max_length=12,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9866c2e6-1d10-497a-aaef-982ff28c7875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ì•„', 'ë”ë¹™', '.', '.', 'ì§„ì§œ', 'ì§œì¦ë‚˜', '##ë„¤ìš”', 'ëª©ì†Œë¦¬'],\n",
       " ['í ',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'í¬ìŠ¤í„°',\n",
       "  '##ë³´ê³ ',\n",
       "  'ì´ˆë”©',\n",
       "  '##ì˜í™”',\n",
       "  '##ì¤„',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'ì˜¤ë²„',\n",
       "  '##ì—°ê¸°',\n",
       "  '##ì¡°ì°¨',\n",
       "  'ê°€ë³',\n",
       "  '##ì§€',\n",
       "  'ì•Š',\n",
       "  '##êµ¬ë‚˜'],\n",
       " ['ë³„ë£¨', 'ì˜€ë‹¤', '.', '.']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784d0f5-c46e-4d1d-a823-2dd85fff30db",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### OpenAIì˜ ìµœì‹  GPT ëª¨ë¸(GPT-3, GPT-3.5, GPT-4)ì€ tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” cl100k_base í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c090016-0108-4365-b7dc-070e78a2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdd8ea79-3bef-4329-8bd5-feb92f1fba49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences: [[54059, 5251, 235, 242, 167, 117, 247, 497, 49011, 226, 17164, 250, 49011, 250, 96064, 251, 61415, 76242, 97, 36811, 38078, 102, 44690, 29102], [169, 251, 254, 1131, 169, 237, 105, 25941, 34961, 42771, 35495, 84415, 67598, 102, 36092, 223, 57390, 59269, 226, 1975, 58368, 80104, 13879, 108, 21121, 93917, 89641, 101, 36609, 29099, 235, 22035, 51796, 89359, 61415], [29099, 226, 53987, 101, 39623, 222, 13447, 497]]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "sentences = [\n",
    "    \"ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\",\n",
    "    \"í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\",\n",
    "    \"ë³„ë£¨ ì˜€ë‹¤..\",\n",
    "]\n",
    "\n",
    "tokenized_sentences = [tokenizer_gpt4.encode(sentence) for sentence in sentences]\n",
    "\n",
    "print(\"Tokenized Sentences:\", tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81743659-54e8-4c3f-ae9d-13758ff502e1",
   "metadata": {},
   "source": [
    "# cl100k_base\n",
    "* Byte-Pair Encoding (BPE) + ìµœì í™”ëœ ì„œë¸Œì›Œë“œ(subword) ë‹¨ìœ„ ë¶„í•  ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë™\n",
    "1.\tì´ì „ ëª¨ë¸(GPT-3)ë³´ë‹¤ ì••ì¶•ëœ í† í°í™” êµ¬ì¡°<br>\n",
    "â†’ ë™ì¼í•œ ë¬¸ì¥ì—ì„œë„ ë” ì ì€ ìˆ˜ì˜ í† í°ì„ ìƒì„± (ë¹„ìš© ì ˆê° íš¨ê³¼)\n",
    "2.\tìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬<br>\n",
    "â†’ ì˜ì–´ì˜ ê²½ìš° â€œHelloâ€ëŠ” [Hello] (1í† í°), â€œrunningâ€ì€ [run, ning] (2í† í°)<br>\n",
    "â†’ í•œêµ­ì–´ì˜ ê²½ìš° â€œì•ˆë…•í•˜ì„¸ìš”â€ëŠ” [ì•ˆë…•í•˜ì„¸ìš”] (1í† í°)<br>\n",
    "3. ì´ì „ í† í¬ë‚˜ì´ì €(p50k_base, r50k_base)ë³´ë‹¤ ë” íš¨ìœ¨ì <br>\n",
    "â†’ ê°™ì€ ë¬¸ì¥ì„ ì ì€ ìˆ˜ì˜ í† í°ìœ¼ë¡œ ë³€í™˜<br>\n",
    "4.\tGPT-4, GPT-3.5 (gpt-3.5-turbo, gpt-4)ì— ìµœì í™”ë¨<br>\n",
    "\n",
    "### ê°œì„ ì \n",
    "1. ë” ì§§ì€ í† í° ê¸¸ì´ â†’ GPT-3 ëŒ€ë¹„ 10~20% ì ì€ í† í° ìˆ˜ ì‚¬ìš©\n",
    "2. í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ìµœì í™” â†’ ê¸°ì¡´ GPT-3ë³´ë‹¤ ì„±ëŠ¥ í–¥ìƒ\n",
    "3. íš¨ìœ¨ì ì¸ íŒ¨ë”© & í† í° êµ¬ì¡° â†’ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ë¶„í•  ê°ì†Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbf32823-d375-48f2-9927-8aade5df8d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ í† í° ID: [31495, 230, 75265, 243, 92245, 0, 74177, 15478, 246, 38295, 254, 168, 242, 101, 20565, 66799, 233, 76242, 97, 36811, 13]\n",
      "ğŸ“Œ ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.\n"
     ]
    }
   ],
   "source": [
    "sentence_ko = \"ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.\"\n",
    "tokenized_ko = tokenizer_gpt4.encode(sentence_ko)\n",
    "\n",
    "print(\"ğŸ“Œ í† í° ID:\", tokenized_ko)\n",
    "print(\"ğŸ“Œ ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸:\", tokenizer_gpt4.decode(tokenized_ko))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ac07f-bd43-4487-92cf-5992497e7597",
   "metadata": {},
   "source": [
    "* â€œì•ˆë…•í•˜ì„¸ìš”â€ â†’ [13392] (1ê°œì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë¨)\n",
    "* â€œë‚ ì”¨ê°€â€ â†’ [22035] (BPEë¥¼ í†µí•´ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì••ì¶•ë¨)\n",
    "* â€œì¢‹ë„¤ìš”â€ â†’ [17606] (í•œ ë©ì–´ë¦¬ë¡œ ì²˜ë¦¬ë¨, ë¶ˆí•„ìš”í•œ ë¶„í•  ì—†ìŒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b61e35-cf3b-4832-91c4-c9ab41366186",
   "metadata": {},
   "source": [
    "ì°¸ê³  ë…¼ë¬¸\n",
    "*  Neural Machine Translation of Rare Words with Subword Units <br>\n",
    "https://arxiv.org/pdf/1508.07909\n",
    " * Japanese and Korean Voice Search<br>\n",
    " https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384fd33-33ea-4732-9e9f-94c23e0555fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
