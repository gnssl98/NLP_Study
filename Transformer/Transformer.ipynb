{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae99594a-bdbf-42b6-a13c-77b2adeda6d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Doit! BERT와 GPT로 배우는 자연어 처리\n",
    "### 저자 이기창\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4c3c6b4-4b2c-409d-8d63-5a62a56b7909",
   "metadata": {},
   "source": [
    "# 언어 모델(Language Model)\n",
    "* 단어 시퀀스에 확률을 부여하는 모델\n",
    "* 단어 시퀀스를 입력받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 모델\n",
    "* i번째로 등장하는 단어를 $w_i$로 표시한다면 n개 단어로 구성된 문장이 해당 언어에서 등장할 확률 -> 언어 모델의 출력을 수식처럼 쓸 수 있다.(결합 확률)\n",
    "* P($w_1,w_2,w_3,...,w_n$)\n",
    "* $P(w_1,w_2,w_3)=P(w_1)\\times P(w_2|w_1)\\times P(w_3|w_1,w_2)$\n",
    "-> 단어 3개로 구성된 문장이 나타나려면 다음 3가지 사건이 동시에 일어나야 한다.\n",
    "  > 첫 번째 단어($w_1$) 등장 <br>\n",
    "  > 첫 번째 단어 등장 후 두 번째 단어($w_2$) 등장<br>\n",
    "  > 첫 번째 단어 등장 후 두 번째 단어 등장 후 세 번째 단어($w_3$) 등장\n",
    "\n",
    "### 전체 단어 시퀀스가 나타날 확률은 이전 단어들이 주어졌을 때 다음 단어가 등장할 확률의 연쇄와 ㄱ타다.\n",
    "### 이렇기에 언어 모델을 이전 단어들이 주어졌을 때 다음 단어가 나타날 확률을 부여하는 모델이라고도 함\n",
    "\n",
    "### $P(w_1,w_2,w_3,...,w_n)= \\prod_{i=1}^{n} P(w_i | w_1, \\dots, w_{i-1})$\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 순방향 언어 모델\n",
    "* 사람이 이해하는 순서대로 계산하는 모델\n",
    "* GPT, ELMo 같은 모델이 이런 방식으로 pretrain을 수행\n",
    "\n",
    "# 역방향 언어 모델\n",
    "* 문장 뒤부터 앞으로 계산하는 모델\n",
    "* ELMo-> 순방향과 역방향 언어 모델 모두 활용하는 기법\n",
    "* https://github.com/allenai/allennlp-models\n",
    "\n",
    "# 넓은 의미의 언어 모델\n",
    "### $P(w|context)$\n",
    "* context(주변 맥락 정보)가 전제된 상태에서 특정 단어(w)가 나타날 조건부 확률을 나타낸다.\n",
    "* 이렇게 정의된 언어 모델은 단어나 단어 시퀀스로 구성된 컨텍스트를 입력받아 특정 단어가 나타날 확률을 출력\n",
    "* context와 맞힐 단어를 어떻게 설정하느냐에 따라 다양하게 변형 가능\n",
    "\n",
    "# 마스크 언어 모델(Masked Language Model)\n",
    "* 학습 대상 문장에 빈칸을 만들어 놓고 해당 빈칸에 올 단어로 적절한 단어가 무엇일지 분류하는 과정으로 학습 (수능 영어 빈칸 맞추기)\n",
    "* BERT가 마스크 언어 모델로 해당\n",
    "* 맞힐 단어 이전 단어들만 참고할 수 있는 순방향 역방향 언어 모델과 달리 마스크 언어 모델은 맞힐 단어를 계산할 때 문장 전체의 맥락을 참고할 수 있다는 장점이 있다.\n",
    "* 양방향 성질 있음\n",
    "\n",
    "# 스킵-그램 모델(skip-gram model)\n",
    "* 어떤 단어 앞뒤에 특정 범위를 정해 두고 이 범위 내에 어떤 단어들이 올지 분류하는 과정으로 학습\n",
    "예시)<br>\n",
    "(어제 카페 갔었어 거기 사람) 많더라<br>\n",
    "<br>\n",
    "어제 (카페 갔었어 거기 사람 많더라)\n",
    "<br>\n",
    "\n",
    "* context로 설정한 단어 주변에 어떤 단어들이 분포해 있는지를 학습\n",
    "* 구글에서 발표한 단어 수준 임베딩 기법인 Word2Vec이 스킵 - 그램 모델 방식으로 학습\n",
    "* https://github.com/tmikolov/word2vec\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 언어 모델의 유용성\n",
    "잘 학습된 언어 모델 -> 어떤 문장이 자연스러운지 가려낼 수 있어 그 자체로 값어치가 있다.\n",
    "<br> 학습 대상 언어와 풍부한 맥락을 포함하고 있다는 점 역시 장점\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/2005.14165\n",
    "\n",
    "* 기계 번역 : P(?|You can't be free from death)\n",
    "* 문법 교정 : P(두시 삼십 이분)>P(이시 서른 두분)\n",
    "* 문장 생성 : P(?|발 없는 말이)\n",
    "\n",
    "트랜스퍼 러닝 -> 대량의 말뭉치로 pretrain한 언어 모델을 문서 분류, 개체명 인식 등 다운스트림 태스크에 적용하면 적은 양의 데이터로도 그 성능을 큰 폭으로 올릴 수 있다. \n",
    "<br>\n",
    "최근에 제안되는 기법들은 pretrain을 마친 딥러닝계열 언어 모델을 바탕으로 할 때가 많음\n",
    "<hr>\n",
    "\n",
    "# 트랜스포머\n",
    "* Sequence to Sequence 모델\n",
    "## Sequence to Sequence\n",
    "특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업\n",
    "기계 번역-> 어떤 언어의 토큰 시퀀스를 다른 언어의 토큰 시퀀스로 변환하는 과제\n",
    "\n",
    "### Enocder \n",
    "* 소스 시퀀스의 정보를 압축하는 과정\n",
    "### Decoder\n",
    "* 타귓 시퀀스를 생성하는 과정\n",
    "\n",
    "\n",
    "<img src=\"ratsgo's NLPBOOK.png\" alt=\"Transformer의 구조\">\n",
    "이미지 출처 : ratsgo's NLPBOOK\n",
    "\n",
    "트랜스포머의 최종 출력, 즉 디코더 출력(그림 3-8에서 다음 토큰 확률)은 타깃 언어의 어휘수만큼의 차원으로 구성된 벡터(vector) 입니다. 이 벡터의 특징은 요소(element)값이 모두 확률이라는 점입니다. 예를 들어 타깃 언어의 어휘가 총 3만 개라고 가정해 보면 디코더 출력은 3만 차원의 벡터입니다. 이 벡터의 요솟값 3만 개 각각은 확률이므로 0 이상 1 이하의 값을 가지며 모두 더하면 1이 됩니다.\n",
    "<br>\n",
    "\n",
    "트랜스포머의 학습은 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률값을 높이는 방식으로 수행됩니다. 이를 나타낸 다음 그림에서 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신합니다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 트랜스포머 블록\n",
    "다음 그림은 트랜스포머의 인코더 가운데 반복되는 요소를 떼어내 다시 나타낸 것입니다.<br>\n",
    "이런 구조를 블록(block) 또는 레이어(layer) 라고 합니다. 트랜스포머의 인코더는 이러한 블록 수십 개를 쌓아서 구성합니다.<br>\n",
    "인코더 -> Multi-head attention, Feedforward neural network, Residual connection, Layer normalization<br>\n",
    "디코더 -> 인코더와 비슷 +masked multi-head attention, 인코더가 보내온 정보와 디코더 입력을 함께 이용해 Multi-head attention을 수행하는 모듈 추가<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# Attention\n",
    "어텐션이 없는 단순 RNN을 사용하면 워낙 초반에 입력된 단어라 모델이 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아질 수 있습니다.<br>\n",
    "\n",
    "어제 카페 갔었어 거기 ... I Went to cafe ...\n",
    "<br>\n",
    "어텐션은 이러한 문제점을 해결하고자 제안- > 디코더 쪽 RNN에 어텐션을 추가<br>\n",
    "어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지를 알려주므로 카페가 소스 시퀀스 초반에 등장하거나, 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있다.\n",
    "<br>\n",
    "\n",
    "# Self Attention\n",
    "시퀀스 요소 가운데 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 Task 수행 성능을 끌어 올린다.\n",
    "<br>\n",
    "어텐션과 셀프 어텐션의 주요 차이를 살펴보면 다음과 같습니다.\n",
    "<br>\n",
    "1. 어텐션은 소스 시퀀스 전체 단어들(어제, 카페, ... ,많더라)과 타깃 시퀀스 단어 하나(cafe) 사이를 연결하는 데 쓰입니다. 반면 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결합니다.\n",
    "\n",
    "2. 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작합니다.\n",
    "\n",
    "3. 타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 셀프 어텐션은 인코더, 디코더 볼록의 개수만큼 반복 수행합니다.\n",
    "<br>\n",
    "### 구성\n",
    "Query, Key, Value의 3가지 요소가 서로 영ㅇ향을 주고받는 구조\n",
    "<br>\n",
    "트랜스포머 블록에는 문장 내 각 단어가 Vector 형태로 입력\n",
    "<br>\n",
    "\n",
    "| **쿼리** | **키** |\n",
    "|---------|------|\n",
    "| 어제    | 0.1 어제  |\n",
    "| <font color=\"red\">카페</font>   | 0.1 카페  |\n",
    "| 갔었어  | 0.2 갔었어 |\n",
    "| 거기    | 0.4 거기  |\n",
    "| 사람    | 0.1 사람  |\n",
    "| 많더라  | 0.1 많더라 |\n",
    "\n",
    "<center> <그림 3-22 셀프 어텐션 계산 예시></center>\n",
    "<br>\n",
    "셀프 어텐션은 쿼리 단어 각각을 대상으로 모든 키 단어의 얼마나 유기적인 관계를 맺는지 그 합이 1인 확률값으로 표현(sofrmax와 비슷)<br>\n",
    "그림을 보면 **카페**라는 쿼리 단어와 가장 관련이 높은 키 단어는 **거기**라는 점(0.4)을 확인할 수 있다.\n",
    "\n",
    "셀프 어텐션 모듈은 이러한 결과에 밸류 벡터들을 ***가중합(weighted sum)*** 하는 방식으로 계산을 마무리한다.\n",
    "이를 다음 수식처럼 나타낼 수 있습니다. 새롭게 만들어지는 ***카페*** 벡터(***$Z_{\\text{카페}}$***)는 문장에 속한 단어와 카페 사이의 관계가 녹아 있습니다.\n",
    "\n",
    "$Z_{\\text{카페}} = 0.1 \\times V_{\\text{어제}} + 0.1 \\times V_{\\text{카페}} + 0.2 \\times V_{\\text{갔었어}} + 0.4 \\times V_{\\text{거기}} + 0.1 \\times V_{\\text{사람}} + 0.1 \\times V_{\\text{많더라}} $\n",
    "\n",
    "<br>\n",
    "이런 방식으로 나머지 단어들도 셀프 어탠션을 가각 수행\n",
    "<br>\n",
    "모든 시퀀스르 대상으로 셀프 어텐션 계산이 끝나면 그 결과를 다음 블록으로 넘긴다.<br>\n",
    "이처럼 트랜스포머 모델은 셀프 어텐션을 블록(Layer) 수만큼 반복\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae2e62-d694-4b0c-978b-95b13a1389d5",
   "metadata": {},
   "source": [
    "# Input Layer\n",
    "모델의 입력을 만드는 계층\n",
    "\n",
    "\n",
    "<img src=\"input_layer.png\">\n",
    "이미지 출처 : ratsgo's NLPBOOK\n",
    "\n",
    "인코더의 입력은 소스 시퀀스의 입력 임베딩에 Positional Encoding을 더해서 만든다.\n",
    "Positional Encoding -> 해당 단어의 위치 정보\n",
    "\n",
    "(1) Query, Key, Value 만들기<br>\n",
    "단어 임베딩 차원 수 (d) = 4, 인코더에 입력된 단어 개수가 3일 경우 X는 3X4 행렬\n",
    "$$ Q = X \\times W_Q $$\n",
    "$$ K = X \\times W_K $$\n",
    "$$ V = X \\times W_V $$\n",
    "\n",
    "ex) Query 만들기\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 & 0 \\\\\n",
    "0 & 2 & 0 & 2 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 2 \\\\\n",
    "2 & 2 & 2 \\\\\n",
    "2 & 1 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이런식으로 Q가 생성되고\n",
    "$$W_Q , W_K, W_V $$ 은 task를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트\n",
    "\n",
    "### 셀프 어텐션의 정의\n",
    "\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(QK^T / \\sqrt d_k)V$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cebc4e7-6d7d-4486-aab3-e690a609e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-1 변수 정의\n",
    "import torch\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0],\n",
    "])\n",
    "w_query = torch.tensor([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0]                        \n",
    "])\n",
    "w_key = torch.tensor([\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [1.0, 1.0, 0.0]                  \n",
    "])\n",
    "w_value = torch.tensor([\n",
    "    [0.0, 2.0, 1.0],\n",
    "    [0.0, 3.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 1.0, 0.0]                        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e852b3c0-19f4-4dd5-aabc-05eda7730618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-2 쿼리, 키, 밸류 만들기\n",
    "keys = torch.matmul(x, w_key)\n",
    "querys = torch.matmul(x, w_query)\n",
    "values = torch.matmul(x, w_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ff3a36-5f55-43a3-ac80-b813f712f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1.],\n",
      "        [4., 4., 0.],\n",
      "        [2., 3., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ea724b-330d-4f01-978a-412f3e15b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-3 어텐션 스코어 만들기\n",
    "attn_scores = torch.matmul(querys, keys.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3b0ea2-4768-478b-9d42-5d6bd94bce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  4.],\n",
       "        [ 4., 16., 12.],\n",
       "        [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af00234a-b2e0-4113-bef9-05970ba7c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-4 소프트맥스 확률값 만들기\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax\n",
    "key_dim_sqrt = np.sqrt(keys.shape[-1])\n",
    "attn_probs = softmax(attn_scores / key_dim_sqrt, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe3a04a-9487-47e5-8146-21b07fc90259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n",
       "        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n",
       "        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb37f4c-fee7-43f3-aeed-a82c1f3efca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-5 소프트맥스 확률과 밸류를 가중합하기\n",
    "weighted_values = torch.matmul(attn_probs, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab26d0d-8fc0-4e4a-8749-95d565047128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8639, 6.3194, 0.5681],\n",
       "        [1.9991, 7.8141, 0.0912],\n",
       "        [1.9926, 7.4796, 0.2453]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d26a93-6e38-4727-b287-43a99517d68a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "셀프 언텐션의 학습 대상 -> Query, key, Value를 만드는 가중치 행렬 $$(W_Q, W_K, W_V)  $$ 이다.\n",
    "<br>\n",
    "코드 예시에서는 w_query, w_key, w_value 이다.<br> 이들을 task(예를 들면 기계 번역)를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트\n",
    "\n",
    "# 멀티 헤드 어텐션(Multi Head Attention)\n",
    "* Self-Attention을 동시에 여러번 수행하는 것\n",
    "\n",
    "<그림 3-27>\n",
    "<img src=\"multihead.png\">\n",
    "출처 : ratsgo's NLPBOOK\n",
    "\n",
    "그림 3-2은 입력 단어 수는 2개, Value의 차원은 3개, Head는 8개인 멀티 헤드 어텐션<br>\n",
    "개별 헤드의 셀프 어탠션 수행 결과는 '입력 단어 수 X Value 차원 수' -> 2x3 크기 행렬<br>\n",
    "8개 Head의 셀프 어텐션 수행 결과를 이어 붙이면 2 X 24 크기의 행렬이 된다.<br>\n",
    "\n",
    "그 후 멀티 해드 어텐션은 개별 헤드의 셀프 어텐션 수행 결과를 이어붙인 행렬에 $$ W^O$$를 행렬곱하여 마무리 된다.\n",
    "<br>\n",
    "$$ W^O$$의 크기는 '셀프 어텐션 수행 결과 행렬의 열 수 X 목표 차원 수' 가 된다.<br>\n",
    "만일 멀티 헤드 어텐션 수행 결과(Z)를 앞의 그림처럼 3차원으로 설정해 두고 싶다면 $$ W^O$$는 24 X 3 크기의 행렬이 되어야 한다.\n",
    "\n",
    "* 멀티 헤드 어텐션의 최종 수행 결과는 '입력 단어  목표 차원 수'이다. \n",
    "* 앞의 그림에서는 입력 단어 2개 각각에 대해 3차원짜리 벡터가 멀티 헤드 어텐션의 최종 결과물로 도출되었다. \n",
    "* 멀티 헤드 어텐션은 인코더, 디코더 블록 모두에 적용된다.\n",
    "\n",
    "<그림 3-28>\n",
    "<img src=\"encoder.png\">\n",
    "출처 : https://github.com/KimYoungHyeop/Transformer-HuggingFace/blob/main/Chapter_3_2022_03_12.ipynb\n",
    "\n",
    "인코더에서 수행되는 셀프 어텐션은 쿼리, 키, 밸류가 모두 소스 시퀀스와 관련된 정보이다.\n",
    "<br>트랜스포머의 학습 과제가 한국어에서 영어로 번역하는 task라면 인코더의 쿼리, 키, 밸류는 모두 한국어가 된다.\n",
    "<br>\n",
    "<img src=\"1.jpeg\"><br>\n",
    "출처 : ratsgo's NLPBOOK<br>\n",
    "다음 그림은 쿼리가 어제일 때 셀프 어텐션을 나타냈습니다.\n",
    "<br>잘 학습된 트랜스포머라면 쿼리, 키로부터 계산한 소프트맥스 확률(코드 3-5의 attn_probs에 대응) 가운데 과거 시제에 해당하는 <b>갔었어, 많더라</b> 등의 단어가 높은 값을 지닐 것이다. 이 확률과 밸류 벡터를 가중합해서 셀프 어텐션 계산을 마친다.\n",
    "<img src=\"2.jpeg\"><br>\n",
    "출처 : ratsgo's NLPBOOK\n",
    "\n",
    "이러한 계산을 <b>갔었어, 거기, 사람, 많더라</b>를 대상으로도 수행합니다. 결국 인코더에서 수행하는 셀프 어텐션은 소스 시퀀스 내의 모든 단어 쌍(pair) 사이의 관계를 고려하게 됩니다.\n",
    "<hr>\n",
    "\n",
    "## 디코더\n",
    "\n",
    "<img src=\"Decoder.jpeg\">\n",
    "출처 : Doit! BERT와 GPT로 배우는 자연어 처리 \n",
    "\n",
    "다음 그림은 인코더와 디코더 블록을 나타낸 그림이다. \n",
    "<br>그림에서도 확인할 수 있듯이 디코더 입력은 인코더 마지막 블록에서 나온 소스 단어 벡터 시퀀스(그림에서 붉은색 실선), 이전 디코더 블록의 수행 결과로 도출된 타깃 단어 벡터 시퀀스(그림에서 파란색 실선)이다.\n",
    "\n",
    "디코더에서 수행되는 셀프 어텐션 -> Masked Multi-Head Attention\n",
    "\n",
    "타깃 언어의 단어 벡터 시퀀스를 계산 대상으로 한다.<br>\n",
    "한국어를 영어로 번역하는 task를 수행하는 트랜스포머 모델이라면 여기서 계산되는 대상은 영어 단어 시퀀스\n",
    "정답을 포함한 타깃 시퀀스의 미래 정보를 셀프 어텐션 계산에서 제외(마스킹) 하게 된다.\n",
    "이를 통해 마스크된 부분을 예측하는 방향으로 모델 학습\n",
    "구체적으로 타깃 시퀀스에 대한 Mask Multi-Head Attention 계산 시 제외 대상 단어들의 Softmax 확률이 0이 되도록 하여 Multi-Head Attention에서도 해당 단어 정보들이 무시되게 하는 방식으로 수행\n",
    "<hr>\n",
    "\n",
    "트랜스포머 모델은 이런 방식으로 말뭉치 전체를 훑어가면서 반복 학습한다. 학습을 마친 모델은 다음처럼 기계 번역을 수행(인퍼런스)한다.\n",
    ">1. 소스 언어(한국어) 문장을 인코더에 입력해 인코더 마지막 블록의 단어 벡터 시퀀스를 추출합니다.\n",
    ">2. 인코더에서 넘어온 소스 언어 문장 정보와 디코더에 타깃 문장 시작을 알리는 스페셜 토큰 (s) 를 넣어서, 타깃 언어(영어)의 첫 번째 토큰을 생성합니다.\n",
    ">   3. 인코더 쪽에서 넘어온 소스 언어 문장 정보와 이전에 생성된 타깃 언어 토큰 시퀀스를 디코더에 넣어서 만든 정보로 타깃 언어의 다음 토큰을 생성합니다.\n",
    ">   4. 생성된 문장 길이가 충분하거나 문장 끝을 알리는 스페셜 토큰 (s) 가 나올 때까지 3번 과정을 반복합니다.\n",
    "\n",
    "한편 (s) 는 보통 타깃 언어 문장 맨 마지막에 붙여서 학습한다. 이 토큰이 나타났다는 것은 모델이 타깃 문장 생성을 마쳤다는 의미이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd52f90-7c0b-4ed7-8d2a-7e09d285ac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56ecc4f3-e7ee-4f27-9a00-c0f66adf895a",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "\n",
    "$f(\\sum_{i} W_i X_i +b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d6f225-d111-4de1-a088-12dce0d98895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-6 피드포워드 뉴럴 네트워크 계산 예시(1)\n",
    "import torch\n",
    "x = torch.tensor([2, 1])\n",
    "w1 = torch.tensor([[3, 2, -4], [2, -3, 1]])\n",
    "b1 = 1\n",
    "w2 = torch.tensor([[-1, 1], [1, 2], [3, 1]])\n",
    "b2 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a5e0a0-6381-4d58-b712-e2cf5b26121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-7 피드포워드 뉴럴 네트워크 계산 예시(2)\n",
    "h_preact = torch.matmul(x, w1) + b1\n",
    "h = torch.nn.functional.relu(h_preact)\n",
    "y = torch.matmul(h, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99ad973e-4acf-4c35-af0d-c80629010248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  2, -6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_preact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff4bacc-c17b-4842-a188-3b86415ee63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b3e3d5e-aaa1-413b-b8c7-027c3862dd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e847c-a3a9-4bc8-8e0d-750b192ac547",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "<img src =\"residual.png\">\n",
    "이미지 출처 : ratsgo's NLPBOOK<br>\n",
    "블록이나 레이어 계산을 건너뛰는 경로를 하나 두는 것을 말한다.\n",
    "<br> 입력을 x, 이번 계산 대상 블록을  F라고 할 때 잔차 연결은 F(x)+x로 간단히 실현한다.\\\n",
    "<br>\n",
    "블록 계산이 계속될 때 잔차 연결을 두는 것은 큰 효과가 있다.\n",
    "<img src=\"residual2.jpeg\">\n",
    "이미지 출처 : ratsgo's NLPBOOK<br>\n",
    "그림에서 오른쪽을 보면 잔차 연결을 두지 않았을 때는 을 연속으로 수행하는 경로 한 가지만 존재했으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생겼다. 다시 말해 모델이 다양한 관점에서 블록 계산을 수행하게 된다.\n",
    "<br>\n",
    "딥러닝 모델은 레이어가 많아지면 학습이 어려운 경향이 있다. 모델을 업데이트하기 위한 신호(그레이디언트)가 전달되는 경로가 길어지기 때문이다. 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 쉽게 하는 효과까지 거둘 수 있다.\n",
    "\n",
    "## Layer Normalization\n",
    "미니 배치의 인스턴스($x$)별로 평균($E[x]$)을 빼주고 표준편차($\\sqrt E[x]$)로 나눠 정규화를 수행하는 기법<br>\n",
    "학습이 안정되고 속도가 빨라지는 효과<br>\n",
    "\n",
    "다음 수식에서 $B$ 와 $r$는 학습 과정에서 업데이트되는 가중치이며, $\\epsilon$은 분모가 0이 되는 걸 방지하려고 더해주는 고정값(보통 1e-5로 설정)\n",
    "$$y=\\frac {x-E[x]}  {\\sqrt {V[x]}+\\epsilon} *r + B $$\n",
    "<수식 레이어 정규화><br>\n",
    "배치의 첫 번째 데이터($x$=[1 2 3])의 평균과 표준편차는 각각 2, 0.8164인데, 이 값들을 바탕으로 수식 3-27과 같은 정규화 수식을 계산하게 된다.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6cb795-12c0-4bc8-a629-18f915207c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#코드 3-8 레이어 정규화 예시\n",
    "import torch\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "m = torch.nn.LayerNorm(input.shape[-1])\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8be9920-c667-45b1-a48d-2e03188b58ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247,  0.0000,  1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66c4213-1423-4cc4-b9b3-e669fd2404fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94fd61a1-efeb-42ec-8b39-8e6b67e08bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6b97-56ac-4c2e-ab15-470754ad8f84",
   "metadata": {},
   "source": [
    "<b>m.weight</b>는 , <b>m.bias</b>는 에 대응하는데, 파이토치의 <b>LayerNorm</b> 객체는 이 두 값을 각각 1과 0으로 초기화한다.\n",
    "<br>다시 말해 학습 초기 레이어 정규화 수행은 배치 인스턴스의 평균을 빼고 표준편차로 나눈 결과에 1을 곱한 후 0을 더한다.<br>\n",
    "이후 학습 과정에서는 task(예를 들면 기계 번역)를 가장 잘 수행하는 방향으로 이 값들을 업데이트한다.\n",
    "\n",
    "# 모델 학습 기법\n",
    "<hr>\n",
    "\n",
    "## 드롭 아웃\n",
    "* 과적합 방지\n",
    "* 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48af3f68-d95e-4aa5-b973-24848b339fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-9 드롭아웃\n",
    "import torch\n",
    "m = torch.nn.Dropout(p=0.2)\n",
    "input = torch.randn(1, 10)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4710f7a3-14e0-4c18-b7c9-c105cf7d8b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3222,  0.8944,  1.6689,  1.5804, -0.0492, -0.3121,  0.5568,  0.1124,\n",
       "          0.1577, -0.1583]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4aba59-f63a-4f2f-b87f-b9796cee52e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4027,  1.1179,  2.0861,  1.9756, -0.0615, -0.3902,  0.6960,  0.1405,\n",
       "          0.1972, -0.1979]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14dd86a-176f-4681-b10c-30beb1f60cdb",
   "metadata": {},
   "source": [
    "* 트랜스포머 모델에서 드롭아웃은 입력 임베딩과 최초 블록 사이, 블록과 블록 사이, 마지막 블록과 출력층 사이 등에 적용\n",
    "* 드롭아웃 비율은 10%($p=0.1$)로 설정하는 것이 일반적\n",
    "* 드롭아웃은 학습 과정에만 적용하고 학습이 끝나고 나서 인퍼런스 과정에서는 적용하지 않는다.\n",
    "<hr>\n",
    "\n",
    "## ADAM Optimizer\n",
    "딥러닝 모델 학습-> 모델 출려과 정답 사이의 error을 최소화하는 방향을 구하고, 이 방향에 맞춰 모델 전체의 Parameter들을 업데이트 하는 과정\n",
    "<br> Error = loss, loss를 최소화하는 방향을 gradient 라고 한다.\n",
    "<br> 오차를 최소화 하는 과정 -> Optimization\n",
    "\n",
    "<br>\n",
    "parameter -> 행렬, 벡터, 스칼라 따위의 모델 구성 요소\n",
    "<br>\n",
    "모델은 순전파(Forword Propagation) -> Error Estimation -> 역전파(Backward Propagation)<br>\n",
    "역전파는 chain Rule에 따라 미분으로 구한다. 이에 따라 미니 배치 단위로 모델 parameter를 업데이트 한다.<br>\n",
    "업데이트 할때 방향과 보폭을 정하는 것 -> Optimizer<br>\n",
    "learning rate\n",
    "\n",
    "<br>\n",
    "\n",
    "# BERT와 GPT\n",
    "<hr>\n",
    "\n",
    "* GPT는 언어 모델입니다.\n",
    "* 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 맞히는 과정에서 pretrain 한다.\n",
    "* 문장 왼쪽부터 오른쪽으로 순차적으로 계산한다는 점에서 일방향(unidirectional)이다.\n",
    "<br>\n",
    "* BERT는 마스크 언어 모델입니다.\n",
    "* 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절할지 맞히는 과정에서 pretrain한다.\n",
    "* 빈칸 앞뒤 문맥을 모두 살필 수 있다는 점에서 양방향(bidirectional) 성격을 가진다.\n",
    "<br>\n",
    "이 때문에 GPT는 문장 생성에, BERT는 문장의 의미를 추출하는 데 강점을 지닌 것으로 알려져 있다.\n",
    "### GPT\n",
    "어제 카페 갔었어 거기 사람 많더라\n",
    "------------------------>\n",
    "### BERT\n",
    "어제 카페 갔었어 [  ] 사람 많더라\n",
    "--------------><----------\n",
    "\n",
    "<br><br>\n",
    "## GPT 구조\n",
    "<hr>\n",
    "GPT는 Transformer의 Encoder을 제외하고 Decoder만 사용한다.\n",
    "<img src=\"gpt.png\">\n",
    "출처 : ratsgo's NLPBOOK\n",
    "디코더 블록을 자세히 보면 인코더 쪽에서 보내오는 정보를 받는 모듈(Multi Head-Attention)가 제거\n",
    "<br>\n",
    "입력 단어 시퀀스가 <b>어제 카페 갔었어 거기 사람 많더라</b>이고 이번이 <b>카페</b>를 맞춰야 하는 상황일 때\n",
    "<br> GPT는 정답 단어 <b>카페</b>를 맞힐 때 <b>어제</b>라는 단어만 참고 가능\n",
    "<br> 따라서 정답 단어 이후의 모든 단어(<b>카페~많더라</b>를 볼 수 없도록 처리\n",
    "<br> 구체적으로 벨류 벡터들을 가중합할 때 참소할 수 없는 단어에 곱하는 점수가 0이 되로록 한다.\n",
    "\n",
    "<b>어제</b>라는 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 한다. \n",
    "<br>그리고 이번 차례의 정답인 카페에 해당하는 확률은 높이고, 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트\n",
    "<br>\n",
    "<b>카페</b>라는 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 한다. \n",
    "<br>그리고 이번 차례의 정답인 <b>갔었어</b>에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트\n",
    "\n",
    "## BERT 구조\n",
    "<hr>\n",
    "<img src=\"bert.png\">\n",
    "출처 : ratsgo's NLPBOOK\n",
    "\n",
    "\n",
    "# Fine Tunning\n",
    "<hr>\n",
    "\n",
    "* Pretrain을 마친 LLM 위에 작은 모듈을 더 쌓아 Task를 수행하는 구조\n",
    "* 문서 분류, 개체명 인식 등 다운스트림 데이터로 업데이트하는 과정\n",
    "\n",
    "\n",
    "### 문장 벡터 활용 : 문서 분류 \n",
    "\n",
    "BERT는 트랜스포머의 인코더 블록(레이어)을 여러 개 쌓은 구조<br>\n",
    "1. 문장을 워드피스로 토큰화한 후 앞뒤에 문장 시작과 끝을 알리는 스페셜 토큰 <b>CLS</b>와 <b>SEP</b>를 각각 추가하고 BERT에 입력 \n",
    "2. BERT 모델의 마지막 블록의 출력 가운데 CLS에 해당하는 벡터를 추출\n",
    "3. 그리고 여기에 간단한 처리를 해서 최종 출력(pooler_output)을 생성 \n",
    "<br>트랜스포머 인코더 블록에서는 모든 단어가 서로 영향을 끼치므로 문장 전체의 의미가 pooler_output 벡터 하나로 응집된다.\n",
    "\n",
    "이렇게 뽑은 pooler_output 벡터에 작은 모듈을 하나 추가해 그 출력이 미리 정해 놓은 범주(예를 들어 긍정, 중립, 부정)가 될 확률이 되도록  한다. \n",
    "<br>학습 과정에서는 BERT와 그 위에 쌓은 작은 모듈을 포함한 전체 모델의 출력이 정답 레이블과 최대한 같아지도록 모델 전체를 업데이트합."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b87658-8565-49ab-afab-c2ef42b9da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#코드 3-12 토크나이저 선언\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b120f73-0c1f-4d75-83b3-df8a83ea058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-13 모델 선언\n",
    "from transformers import BertConfig, BertModel\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    \"beomi/kcbert-base\"\n",
    ")\n",
    "model = BertModel.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afd07e7-02ec-4b29-8d05-3093fd322c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 300,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained_model_config에는 BERT 모델을 프리트레인할 때 설정했던 내용이 담겨 있다. \n",
    "#블록(레이어)수는 12개, 헤드 수는 12개, 어휘 집합 크기는 3만 개 .\n",
    "pretrained_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1bbc05-747d-4a6e-9855-d147f2fa92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#코드 3-14 입력값 만들기\n",
    "sentences = [\"안녕하세요\", \"하이!\"]\n",
    "features = tokenizer(\n",
    "    sentences,\n",
    "    max_length=10,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39d9bb5-f2b2-43a9-9a5e-20186f3186e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 5, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78c82d-3b7b-4439-a393-a7b2dadff45b",
   "metadata": {},
   "source": [
    "2개의 입력 문장 각각에 대해 워드피스 토큰화를 수행한 뒤 이를 토큰 인덱스로 변환한 결과가 input_ids \n",
    "<br>BERT 모델은 문장 시작에 CLS, 끝에 SEP라는 스페셜 토큰을 추가하므로 문장 2개 모두 앞뒤에 이들 토큰에 대응하는 인덱스 2, 3이 붙었음을 확인할 수 있다.\n",
    "<br><br>\n",
    "토큰 최대 길이(max_length)를 10으로 설정하고, 토큰 길이가 이보다 짧으면 최대 길이에 맞게 패딩(0)을 주고(padding=\"max_length\"), 길면 자르도록(truncation=True) 설정했으므로 input_ids의 길이는 두 문장 모두 10인 것을 확인할 수 있다.\n",
    "<br><br>\n",
    "한편 attention_mask는 패딩이 아닌 토큰이 1, 패딩인 토큰이 0으로 실제 토큰이 자리하는지 아닌지의 정보를 나타낸다. \n",
    "<br>token_type_ids는 세그먼트 정보로 지금처럼 각각이 1개의 문장으로 구성됐을 때는 모두 0이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf450db-912a-44fe-8dd4-308a7cbf36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#코드 3-15 피처를 토치 텐서로 변환\n",
    "features = {k: torch.tensor(v) for k, v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d43eeb-3e4c-4d96-a85d-b2f85e5a4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 3-16 임베딩 계산하기\n",
    "outputs = model(**features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e393d3-856c-4d5c-8636-33b81b2ab5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6969, -0.8248,  1.7512,  ..., -0.3732,  0.7399,  1.1907],\n",
       "         [-1.4803, -0.4398,  0.9444,  ..., -0.7405, -0.0211,  1.3064],\n",
       "         [-1.4299, -0.5033, -0.2069,  ...,  0.1285, -0.2611,  1.6057],\n",
       "         ...,\n",
       "         [-1.4406,  0.3431,  1.4043,  ..., -0.0565,  0.8450, -0.2170],\n",
       "         [-1.3625, -0.2404,  1.1757,  ...,  0.8876, -0.1054,  0.0734],\n",
       "         [-1.4244,  0.1518,  1.2920,  ...,  0.0245,  0.7572,  0.0080]],\n",
       "\n",
       "        [[ 0.9371, -1.4749,  1.7351,  ..., -0.3426,  0.8050,  0.4031],\n",
       "         [ 1.6095, -1.7269,  2.7936,  ...,  0.3100, -0.4787, -1.2491],\n",
       "         [ 0.4861, -0.4569,  0.5712,  ..., -0.1769,  1.1253, -0.2756],\n",
       "         ...,\n",
       "         [ 1.2362, -0.6181,  2.0906,  ...,  1.3677,  0.8132, -0.2742],\n",
       "         [ 0.5409, -0.9652,  1.6237,  ...,  1.2395,  0.9185,  0.1782],\n",
       "         [ 1.9001, -0.5859,  3.0156,  ...,  1.4967,  0.1924, -0.4448]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e220a34-ef0d-48bf-9cb1-a074cba27ef6",
   "metadata": {},
   "source": [
    "outputs.last_hidden_state.shape-> [2,10,768]\n",
    "<br>문장 2개에 속한 각각의 토큰(시퀀스 길이는 10)이 768차원짜리의 벡터로 변환.\n",
    "<br>\n",
    "<br>\n",
    "자연어 처리에서는 보통 [배치 크기, 토큰 수, 토큰 벡터 차원]이라는 3차원 텐서를 사용합니다.\n",
    "<br><br>\n",
    "안녕하세요만 따로 떼어서 그 계산 과정을 나타낸 것입니다. outputs.last_hidden_state는 이 그림에서 노란색 점선으로 표기한 벡터들에 대응합니다. 이러한 결과는 개체명 인식 과제처럼 단어별로 수행해야 하는 task에 활용됩니다.\n",
    "한편 그림 3-67에서 패딩(0)에 해당하는 토큰들은 셀프 어텐션에서의 상호작용이 제한됩니다. 해당 토큰의 attention_mask가 0이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dadeb9fd-b0da-4810-bbe8-fb8af8a2c962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9221,  0.2969, -0.0110,  ...,  0.4291,  0.0311, -0.9955]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d259675-de88-4aa7-917a-c8409598f94f",
   "metadata": {},
   "source": [
    "[2, 768] -> 문장 2개가 각각 768차원짜리의 벡터로 변환됐다는 의미. \n",
    "<br><br>이들은 BERT의 마지막 레이어 CLS 벡터에 다음 그림과 같은 처리를 한 결과입니다. \n",
    "<br>이러한 결과는 문서 분류 과제처럼 문장 전체를 벡터 하나로 변환한 뒤 이 벡터에 어떤 계산을 수행하는 task에 활용됩니다.\n",
    "\n",
    "<br><br>\n",
    "pooler_output을 만드는 과정-> 마지막 레이어 CLS 벡터(h)에 행렬 하나를 곱한 뒤 해당 벡터 요소값 각각에 하이퍼볼릭탄젠트(tanh)를 취한다. \n",
    "<br>\n",
    "자연어를 벡터로 바꾼 결과를 Embedding 또는 Representation이라고 한다.\n",
    "\n",
    "<br>\n",
    "The Annotated Transformer: nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "The Illustrated Transformer: jalammer.github/io/illustrated-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f65cd-6f91-4b17-b56c-608a1028665c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
