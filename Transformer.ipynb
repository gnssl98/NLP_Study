{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae99594a-bdbf-42b6-a13c-77b2adeda6d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Doit! BERT와 GPT로 배우는 자연어 처리\n",
    "### 저자 이기창\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3c6b4-4b2c-409d-8d63-5a62a56b7909",
   "metadata": {},
   "source": [
    "# 언어 모델(Language Model)\n",
    "* 단어 시퀀스에 확률을 부여하는 모델\n",
    "* 단어 시퀀스를 입력받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 모델\n",
    "* i번째로 등장하는 단어를 $w_i$로 표시한다면 n개 단어로 구성된 문장이 해당 언어에서 등장할 확률 -> 언어 모델의 출력을 수식처럼 쓸 수 있다.(결합 확률)\n",
    "* P($w_1,w_2,w_3,...,w_n$)\n",
    "* $P(w_1,w_2,w_3)=P(w_1)\\times P(w_2|w_1)\\times P(w_3|w_1,w_2)$\n",
    "-> 단어 3개로 구성된 문장이 나타나려면 다음 3가지 사건이 동시에 일어나야 한다.\n",
    "  > 첫 번째 단어($w_1$) 등장 <br>\n",
    "  > 첫 번째 단어 등장 후 두 번째 단어($w_2$) 등장<br>\n",
    "  > 첫 번째 단어 등장 후 두 번째 단어 등장 후 세 번째 단어($w_3$) 등장\n",
    "\n",
    "### 전체 단어 시퀀스가 나타날 확률은 이전 단어들이 주어졌을 때 다음 단어가 등장할 확률의 연쇄와 ㄱ타다.\n",
    "### 이렇기에 언어 모델을 이전 단어들이 주어졌을 때 다음 단어가 나타날 확률을 부여하는 모델이라고도 함\n",
    "\n",
    "### $P(w_1,w_2,w_3,...,w_n)= \\prod_{i=1}^{n} P(w_i | w_1, \\dots, w_{i-1})$\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 순방향 언어 모델\n",
    "* 사람이 이해하는 순서대로 계산하는 모델\n",
    "* GPT, ELMo 같은 모델이 이런 방식으로 pretrain을 수행\n",
    "\n",
    "# 역방향 언어 모델\n",
    "* 문장 뒤부터 앞으로 계산하는 모델\n",
    "* ELMo-> 순방향과 역방향 언어 모델 모두 활용하는 기법\n",
    "* https://github.com/allenai/allennlp-models\n",
    "\n",
    "# 넓은 의미의 언어 모델\n",
    "### $P(w|context)$\n",
    "* context(주변 맥락 정보)가 전제된 상태에서 특정 단어(w)가 나타날 조건부 확률을 나타낸다.\n",
    "* 이렇게 정의된 언어 모델은 단어나 단어 시퀀스로 구성된 컨텍스트를 입력받아 특정 단어가 나타날 확률을 출력\n",
    "* context와 맞힐 단어를 어떻게 설정하느냐에 따라 다양하게 변형 가능\n",
    "\n",
    "# 마스크 언어 모델(Masked Language Model)\n",
    "* 학습 대상 문장에 빈칸을 만들어 놓고 해당 빈칸에 올 단어로 적절한 단어가 무엇일지 분류하는 과정으로 학습 (수능 영어 빈칸 맞추기)\n",
    "* BERT가 마스크 언어 모델로 해당\n",
    "* 맞힐 단어 이전 단어들만 참고할 수 있는 순방향 역방향 언어 모델과 달리 마스크 언어 모델은 맞힐 단어를 계산할 때 문장 전체의 맥락을 참고할 수 있다는 장점이 있다.\n",
    "* 양방향 성질 있음\n",
    "\n",
    "# 스킵-그램 모델(skip-gram model)\n",
    "* 어떤 단어 앞뒤에 특정 범위를 정해 두고 이 범위 내에 어떤 단어들이 올지 분류하는 과정으로 학습\n",
    "예시)<br>\n",
    "(어제 카페 갔었어 거기 사람) 많더라<br>\n",
    "<br>\n",
    "어제 (카페 갔었어 거기 사람 많더라)\n",
    "<br>\n",
    "\n",
    "* context로 설정한 단어 주변에 어떤 단어들이 분포해 있는지를 학습\n",
    "* 구글에서 발표한 단어 수준 임베딩 기법인 Word2Vec이 스킵 - 그램 모델 방식으로 학습\n",
    "* https://github.com/tmikolov/word2vec\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 언어 모델의 유용성\n",
    "잘 학습된 언어 모델 -> 어떤 문장이 자연스러운지 가려낼 수 있어 그 자체로 값어치가 있다.\n",
    "<br> 학습 대상 언어와 풍부한 맥락을 포함하고 있다는 점 역시 장점\n",
    "\n",
    "\n",
    "* 기계 번역 : P(?|You can't be free from death)\n",
    "* 문법 교정 : P(두시 삼십 이분)>P(이시 서른 두분)\n",
    "* 문장 생성 : P(?|발 없는 말이)\n",
    "\n",
    "트랜스퍼 러닝 -> 대량의 말뭉치로 pretrain한 언어 모델을 문서 분류, 개체명 인식 등 다운스트림 태스크에 적용하면 적은 양의 데이터로도 그 성능을 큰 폭으로 올릴 수 있다. \n",
    "<br>\n",
    "최근에 제안되는 기법들은 pretrain을 마친 딥러닝계열 언어 모델을 바탕으로 할 때가 많음\n",
    "<hr>\n",
    "\n",
    "# 트랜스포머\n",
    "* Sequence to Sequence 모델\n",
    "### Sequence to Sequence\n",
    "특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업\n",
    "기계 번역-> 어떤 언어의 토큰 시퀀스를 다른 언어의 토큰 시퀀스로 변환하는 과제\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682ca05-0a13-4405-9d93-224cf381e4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
