{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae99594a-bdbf-42b6-a13c-77b2adeda6d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Doit! BERT와 GPT로 배우는 자연어 처리\n",
    "### 저자 이기창\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4c3c6b4-4b2c-409d-8d63-5a62a56b7909",
   "metadata": {},
   "source": [
    "# 언어 모델(Language Model)\n",
    "* 단어 시퀀스에 확률을 부여하는 모델\n",
    "* 단어 시퀀스를 입력받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 모델\n",
    "* i번째로 등장하는 단어를 $w_i$로 표시한다면 n개 단어로 구성된 문장이 해당 언어에서 등장할 확률 -> 언어 모델의 출력을 수식처럼 쓸 수 있다.(결합 확률)\n",
    "* P($w_1,w_2,w_3,...,w_n$)\n",
    "* $P(w_1,w_2,w_3)=P(w_1)\\times P(w_2|w_1)\\times P(w_3|w_1,w_2)$\n",
    "-> 단어 3개로 구성된 문장이 나타나려면 다음 3가지 사건이 동시에 일어나야 한다.\n",
    "  > 첫 번째 단어($w_1$) 등장 <br>\n",
    "  > 첫 번째 단어 등장 후 두 번째 단어($w_2$) 등장<br>\n",
    "  > 첫 번째 단어 등장 후 두 번째 단어 등장 후 세 번째 단어($w_3$) 등장\n",
    "\n",
    "### 전체 단어 시퀀스가 나타날 확률은 이전 단어들이 주어졌을 때 다음 단어가 등장할 확률의 연쇄와 ㄱ타다.\n",
    "### 이렇기에 언어 모델을 이전 단어들이 주어졌을 때 다음 단어가 나타날 확률을 부여하는 모델이라고도 함\n",
    "\n",
    "### $P(w_1,w_2,w_3,...,w_n)= \\prod_{i=1}^{n} P(w_i | w_1, \\dots, w_{i-1})$\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 순방향 언어 모델\n",
    "* 사람이 이해하는 순서대로 계산하는 모델\n",
    "* GPT, ELMo 같은 모델이 이런 방식으로 pretrain을 수행\n",
    "\n",
    "# 역방향 언어 모델\n",
    "* 문장 뒤부터 앞으로 계산하는 모델\n",
    "* ELMo-> 순방향과 역방향 언어 모델 모두 활용하는 기법\n",
    "* https://github.com/allenai/allennlp-models\n",
    "\n",
    "# 넓은 의미의 언어 모델\n",
    "### $P(w|context)$\n",
    "* context(주변 맥락 정보)가 전제된 상태에서 특정 단어(w)가 나타날 조건부 확률을 나타낸다.\n",
    "* 이렇게 정의된 언어 모델은 단어나 단어 시퀀스로 구성된 컨텍스트를 입력받아 특정 단어가 나타날 확률을 출력\n",
    "* context와 맞힐 단어를 어떻게 설정하느냐에 따라 다양하게 변형 가능\n",
    "\n",
    "# 마스크 언어 모델(Masked Language Model)\n",
    "* 학습 대상 문장에 빈칸을 만들어 놓고 해당 빈칸에 올 단어로 적절한 단어가 무엇일지 분류하는 과정으로 학습 (수능 영어 빈칸 맞추기)\n",
    "* BERT가 마스크 언어 모델로 해당\n",
    "* 맞힐 단어 이전 단어들만 참고할 수 있는 순방향 역방향 언어 모델과 달리 마스크 언어 모델은 맞힐 단어를 계산할 때 문장 전체의 맥락을 참고할 수 있다는 장점이 있다.\n",
    "* 양방향 성질 있음\n",
    "\n",
    "# 스킵-그램 모델(skip-gram model)\n",
    "* 어떤 단어 앞뒤에 특정 범위를 정해 두고 이 범위 내에 어떤 단어들이 올지 분류하는 과정으로 학습\n",
    "예시)<br>\n",
    "(어제 카페 갔었어 거기 사람) 많더라<br>\n",
    "<br>\n",
    "어제 (카페 갔었어 거기 사람 많더라)\n",
    "<br>\n",
    "\n",
    "* context로 설정한 단어 주변에 어떤 단어들이 분포해 있는지를 학습\n",
    "* 구글에서 발표한 단어 수준 임베딩 기법인 Word2Vec이 스킵 - 그램 모델 방식으로 학습\n",
    "* https://github.com/tmikolov/word2vec\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 언어 모델의 유용성\n",
    "잘 학습된 언어 모델 -> 어떤 문장이 자연스러운지 가려낼 수 있어 그 자체로 값어치가 있다.\n",
    "<br> 학습 대상 언어와 풍부한 맥락을 포함하고 있다는 점 역시 장점\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/2005.14165\n",
    "\n",
    "* 기계 번역 : P(?|You can't be free from death)\n",
    "* 문법 교정 : P(두시 삼십 이분)>P(이시 서른 두분)\n",
    "* 문장 생성 : P(?|발 없는 말이)\n",
    "\n",
    "트랜스퍼 러닝 -> 대량의 말뭉치로 pretrain한 언어 모델을 문서 분류, 개체명 인식 등 다운스트림 태스크에 적용하면 적은 양의 데이터로도 그 성능을 큰 폭으로 올릴 수 있다. \n",
    "<br>\n",
    "최근에 제안되는 기법들은 pretrain을 마친 딥러닝계열 언어 모델을 바탕으로 할 때가 많음\n",
    "<hr>\n",
    "\n",
    "# 트랜스포머\n",
    "* Sequence to Sequence 모델\n",
    "## Sequence to Sequence\n",
    "특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업\n",
    "기계 번역-> 어떤 언어의 토큰 시퀀스를 다른 언어의 토큰 시퀀스로 변환하는 과제\n",
    "\n",
    "### Enocder \n",
    "* 소스 시퀀스의 정보를 압축하는 과정\n",
    "### Decoder\n",
    "* 타귓 시퀀스를 생성하는 과정\n",
    "\n",
    "\n",
    "<img src=\"ratsgo's NLPBOOK.png\" alt=\"Transformer의 구조\">\n",
    "이미지 출처 : ratsgo's NLPBOOK\n",
    "\n",
    "트랜스포머의 최종 출력, 즉 디코더 출력(그림 3-8에서 다음 토큰 확률)은 타깃 언어의 어휘수만큼의 차원으로 구성된 벡터(vector) 입니다. 이 벡터의 특징은 요소(element)값이 모두 확률이라는 점입니다. 예를 들어 타깃 언어의 어휘가 총 3만 개라고 가정해 보면 디코더 출력은 3만 차원의 벡터입니다. 이 벡터의 요솟값 3만 개 각각은 확률이므로 0 이상 1 이하의 값을 가지며 모두 더하면 1이 됩니다.\n",
    "<br>\n",
    "\n",
    "트랜스포머의 학습은 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률값을 높이는 방식으로 수행됩니다. 이를 나타낸 다음 그림에서 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신합니다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# 트랜스포머 블록\n",
    "다음 그림은 트랜스포머의 인코더 가운데 반복되는 요소를 떼어내 다시 나타낸 것입니다.<br>\n",
    "이런 구조를 블록(block) 또는 레이어(layer) 라고 합니다. 트랜스포머의 인코더는 이러한 블록 수십 개를 쌓아서 구성합니다.<br>\n",
    "인코더 -> Multi-head attention, Feedforward neural network, Residual connection, Layer normalization<br>\n",
    "디코더 -> 인코더와 비슷 +masked multi-head attention, 인코더가 보내온 정보와 디코더 입력을 함께 이용해 Multi-head attention을 수행하는 모듈 추가<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# Attention\n",
    "어텐션이 없는 단순 RNN을 사용하면 워낙 초반에 입력된 단어라 모델이 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아질 수 있습니다.<br>\n",
    "\n",
    "어제 카페 갔었어 거기 ... I Went to cafe ...\n",
    "<br>\n",
    "어텐션은 이러한 문제점을 해결하고자 제안- > 디코더 쪽 RNN에 어텐션을 추가<br>\n",
    "어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지를 알려주므로 카페가 소스 시퀀스 초반에 등장하거나, 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있다.\n",
    "<br>\n",
    "\n",
    "# Self Attention\n",
    "시퀀스 요소 가운데 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 Task 수행 성능을 끌어 올린다.\n",
    "<br>\n",
    "어텐션과 셀프 어텐션의 주요 차이를 살펴보면 다음과 같습니다.\n",
    "<br>\n",
    "1. 어텐션은 소스 시퀀스 전체 단어들(어제, 카페, ... ,많더라)과 타깃 시퀀스 단어 하나(cafe) 사이를 연결하는 데 쓰입니다. 반면 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결합니다.\n",
    "\n",
    "2. 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작합니다.\n",
    "\n",
    "3. 타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 셀프 어텐션은 인코더, 디코더 볼록의 개수만큼 반복 수행합니다.\n",
    "<br>\n",
    "### 구성\n",
    "Query, Key, Value의 3가지 요소가 서로 영ㅇ향을 주고받는 구조\n",
    "<br>\n",
    "트랜스포머 블록에는 문장 내 각 단어가 Vector 형태로 입력\n",
    "<br>\n",
    "\n",
    "| **쿼리** | **키** |\n",
    "|---------|------|\n",
    "| 어제    | 0.1 어제  |\n",
    "| <font color=\"red\">카페</font>   | 0.1 카페  |\n",
    "| 갔었어  | 0.2 갔었어 |\n",
    "| 거기    | 0.4 거기  |\n",
    "| 사람    | 0.1 사람  |\n",
    "| 많더라  | 0.1 많더라 |\n",
    "\n",
    "<center> <그림 3-22 셀프 어텐션 계산 예시></center>\n",
    "<br>\n",
    "셀프 어텐션은 쿼리 단어 각각을 대상으로 모든 키 단어의 얼마나 유기적인 관계를 맺는지 그 합이 1인 확률값으로 표현(sofrmax와 비슷)<br>\n",
    "그림을 보면 **카페**라는 쿼리 단어와 가장 관련이 높은 키 단어는 **거기**라는 점(0.4)을 확인할 수 있다.\n",
    "\n",
    "셀프 어텐션 모듈은 이러한 결과에 밸류 벡터들을 ***가중합(weighted sum)*** 하는 방식으로 계산을 마무리한다.\n",
    "이를 다음 수식처럼 나타낼 수 있습니다. 새롭게 만들어지는 ***카페*** 벡터(***$Z_{\\text{카페}}$***)는 문장에 속한 단어와 카페 사이의 관계가 녹아 있습니다.\n",
    "\n",
    "$Z_{\\text{카페}} = 0.1 \\times V_{\\text{어제}} + 0.1 \\times V_{\\text{카페}} + 0.2 \\times V_{\\text{갔었어}} + 0.4 \\times V_{\\text{거기}} + 0.1 \\times V_{\\text{사람}} + 0.1 \\times V_{\\text{많더라}} $\n",
    "\n",
    "<br>\n",
    "이런 방식으로 나머지 단어들도 셀프 어탠션을 가각 수행\n",
    "<br>\n",
    "모든 시퀀스르 대상으로 셀프 어텐션 계산이 끝나면 그 결과를 다음 블록으로 넘긴다.<br>\n",
    "이처럼 트랜스포머 모델은 셀프 어텐션을 블록(Layer) 수만큼 반복\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae2e62-d694-4b0c-978b-95b13a1389d5",
   "metadata": {},
   "source": [
    "# Input Layer\n",
    "모델의 입력을 만드는 계층\n",
    "\n",
    "\n",
    "<img src=\"input_layer.png\">\n",
    "이미지 출처 : ratsgo's NLPBOOK\n",
    "\n",
    "인코더의 입력은 소스 시퀀스의 입력 임베딩에 Positional Encoding을 더해서 만든다.\n",
    "Positional Encoding -> 해당 단어의 위치 정보\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebc4e7-6d7d-4486-aab3-e690a609e9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
