{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f602cf3-d1f4-4760-a5b4-1fd818d619b5",
   "metadata": {},
   "source": [
    "## NSMC(Naver Sentiment Movie Corpis\n",
    "네이버 영화 리뷰 말뭉치<br>\n",
    "from Korpora import Korpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6539089-3c70-40dd-bb6b-c0ef87fd1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ClassificationTrainArguments:\n",
    "    pretrained_model_name: str\n",
    "    downstream_corpus_name: str\n",
    "    downstream_corpus_root_dir: str\n",
    "    downstream_model_dir: str\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",#사전 학습된 모델, Hugging Face의 모델 허브\n",
    "    downstream_corpus_name=\"nsmc\", #네이버 corpus 다운\n",
    "    downstream_corpus_root_dir=\"./data\",\n",
    "    downstream_model_dir=\"./model\",\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cdbcd5-1da3-48fd-81c7-e8115e782ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 47.4MB/s]                                                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 28.9MB/s]                                                             \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0524a69-36e4-4261-8f88-9593512890e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#kcbert-base 모델 준비\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertConfig, BertForSequenceClassification\n\u001b[0;32m      3\u001b[0m pretrained_model_config \u001b[38;5;241m=\u001b[39m BertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m.\u001b[39mpretrained_model_name,\n\u001b[0;32m      5\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      8\u001b[0m     args\u001b[38;5;241m.\u001b[39mpretrained_model_name,\n\u001b[0;32m      9\u001b[0m     config\u001b[38;5;241m=\u001b[39mpretrained_model_config,\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#kcbert-base 모델 준비\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=2,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7b7f4-1d19-4c32-a8fd-faf7d9b07856",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "토큰화 수행 프로그램<br>\n",
    "kcbert-base 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45301cd-f7d1-4b03-bcd5-288a74ce12cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#kcbert-base 토크나이저 준비\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m.\u001b[39mpretrained_model_name,\n\u001b[0;32m      5\u001b[0m     do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m#대소문자 유지, True면 모든 입력 소문자로 변환\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ) \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#kcbert-base 토크나이저 준비\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False, #대소문자 유지, True면 모든 입력 소문자로 변환\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f43528-f66b-45cd-8450-29785e6f840b",
   "metadata": {},
   "source": [
    "# Pytorch's Data Loader\n",
    "\n",
    "* 파이토치로 딥러닝 모델을 만들려면 반드시 정의해야 한다.\n",
    "* 데이터를 배치(batch)단위로 모델에 밀어 넣어주는 역할\n",
    "* 전체 데이터 가운데 일부 인스턴스를 뽑아 배치를 구성\n",
    "* 데이터셋은 데이터 로더의 구성 요소 중 하나\n",
    "* 데이터셋은 여러 인스턴스를 보유\n",
    "\n",
    "데이터 로더 > 데이터셋 > 인스턴스\n",
    "\n",
    "* batch는 그 모양이 고정적이어야 할 때가 많다. -> 문장들의 토큰(input_ids) 개수가 같아야 한다.\n",
    "\n",
    "그래서 batch의 shape을 동일하게 만들어 주는 과정을 collate라고 한다.\n",
    "\n",
    "### Collate\n",
    "* list -> pytorch의 tensor로 변환\n",
    "* batch size 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32b9aa0-2f5a-4883-a2b9-ca15b520932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# NSMC 데이터 다운로드 및 로드\n",
    "data_dir = \"./data/nsmc\"  # 데이터 저장 경로\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_file = f\"{data_dir}/ratings_train.txt\"\n",
    "test_file = f\"{data_dir}/ratings_test.txt\"\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    from Korpora import Korpora\n",
    "    Korpora.fetch(\"nsmc\", root_dir=data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce351a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 1. 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# ✅ 2. 데이터 불러오기 (필요하면 샘플링하여 테스트)\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\").dropna().sample(5000)  # 일부만 사용\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\").dropna().sample(1000)\n",
    "\n",
    "# ✅ 3. 최적화된 Dataset (미리 토큰화 적용)\n",
    "class NsmcDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df[\"document\"].astype(str).tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "\n",
    "        # 🔥 미리 한 번에 토큰화하여 저장\n",
    "        self.encodings = self.tokenizer(\n",
    "            self.texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# ✅ 4. 데이터셋 생성\n",
    "train_dataset = NsmcDataset(train_df, tokenizer)\n",
    "test_dataset = NsmcDataset(test_df, tokenizer)\n",
    "\n",
    "# ✅ 5. 최적화된 DataLoader 설정\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,  # RandomSampler 제거\n",
    "    drop_last=False,\n",
    "    num_workers=0,  # 테스트 결과에 따라 조절 (4~6 추천)\n",
    "    pin_memory=True,  # GPU 속도 향상\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# ✅ 6. 속도 테스트\n",
    "import time\n",
    "start_time = time.time()\n",
    "sample = next(iter(train_dataloader))\n",
    "print(f\"로딩 시간: {time.time() - start_time:.2f}초\")\n",
    "\n",
    "# ✅ 데이터 샘플 확인\n",
    "print(sample[\"input_ids\"].shape)  # torch.Size([32, 128])\n",
    "print(sample[\"label\"])  # tensor([0, 1, 1, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2124a-ca1b-4ca5-bf3a-84a65647421f",
   "metadata": {},
   "source": [
    "### Pytorch Lightning\n",
    "https://minjoo-happy-blog.tistory.com/140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605cf60-4950-4efa-9777-f5b7b8d55b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AdamW\n",
    "\n",
    "# PyTorch Lightning을 사용한 감성 분석 모델 정의\n",
    "class SentimentClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "task = SentimentClassificationTask(model, learning_rate=args.learning_rate)\n",
    "\n",
    "# PyTorch Lightning Trainer 설정\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,  # 학습 횟수\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# 모델 학습 실행\n",
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders=train_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
