{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f602cf3-d1f4-4760-a5b4-1fd818d619b5",
   "metadata": {},
   "source": [
    "## NSMC(Naver Sentiment Movie Corpis\n",
    "ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë§ë­‰ì¹˜<br>\n",
    "from Korpora import Korpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6539089-3c70-40dd-bb6b-c0ef87fd1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ClassificationTrainArguments:\n",
    "    pretrained_model_name: str\n",
    "    downstream_corpus_name: str\n",
    "    downstream_corpus_root_dir: str\n",
    "    downstream_model_dir: str\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",#ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸, Hugging Faceì˜ ëª¨ë¸ í—ˆë¸Œ\n",
    "    downstream_corpus_name=\"nsmc\", #ë„¤ì´ë²„ corpus ë‹¤ìš´\n",
    "    downstream_corpus_root_dir=\"./data\",\n",
    "    downstream_model_dir=\"./model\",\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cdbcd5-1da3-48fd-81c7-e8115e782ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 47.4MB/s]                                                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 28.9MB/s]                                                             \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0524a69-36e4-4261-8f88-9593512890e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#kcbert-base ëª¨ë¸ ì¤€ë¹„\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertConfig, BertForSequenceClassification\n\u001b[0;32m      3\u001b[0m pretrained_model_config \u001b[38;5;241m=\u001b[39m BertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m.\u001b[39mpretrained_model_name,\n\u001b[0;32m      5\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      8\u001b[0m     args\u001b[38;5;241m.\u001b[39mpretrained_model_name,\n\u001b[0;32m      9\u001b[0m     config\u001b[38;5;241m=\u001b[39mpretrained_model_config,\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#kcbert-base ëª¨ë¸ ì¤€ë¹„\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=2,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7b7f4-1d19-4c32-a8fd-faf7d9b07856",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "í† í°í™” ìˆ˜í–‰ í”„ë¡œê·¸ë¨<br>\n",
    "kcbert-base ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45301cd-f7d1-4b03-bcd5-288a74ce12cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#kcbert-base í† í¬ë‚˜ì´ì € ì¤€ë¹„\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m.\u001b[39mpretrained_model_name,\n\u001b[0;32m      5\u001b[0m     do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m#ëŒ€ì†Œë¬¸ì ìœ ì§€, Trueë©´ ëª¨ë“  ì…ë ¥ ì†Œë¬¸ìë¡œ ë³€í™˜\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ) \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#kcbert-base í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False, #ëŒ€ì†Œë¬¸ì ìœ ì§€, Trueë©´ ëª¨ë“  ì…ë ¥ ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f43528-f66b-45cd-8450-29785e6f840b",
   "metadata": {},
   "source": [
    "# Pytorch's Data Loader\n",
    "\n",
    "* íŒŒì´í† ì¹˜ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ë ¤ë©´ ë°˜ë“œì‹œ ì •ì˜í•´ì•¼ í•œë‹¤.\n",
    "* ë°ì´í„°ë¥¼ ë°°ì¹˜(batch)ë‹¨ìœ„ë¡œ ëª¨ë¸ì— ë°€ì–´ ë„£ì–´ì£¼ëŠ” ì—­í• \n",
    "* ì „ì²´ ë°ì´í„° ê°€ìš´ë° ì¼ë¶€ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë½‘ì•„ ë°°ì¹˜ë¥¼ êµ¬ì„±\n",
    "* ë°ì´í„°ì…‹ì€ ë°ì´í„° ë¡œë”ì˜ êµ¬ì„± ìš”ì†Œ ì¤‘ í•˜ë‚˜\n",
    "* ë°ì´í„°ì…‹ì€ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë³´ìœ \n",
    "\n",
    "ë°ì´í„° ë¡œë” > ë°ì´í„°ì…‹ > ì¸ìŠ¤í„´ìŠ¤\n",
    "\n",
    "* batchëŠ” ê·¸ ëª¨ì–‘ì´ ê³ ì •ì ì´ì–´ì•¼ í•  ë•Œê°€ ë§ë‹¤. -> ë¬¸ì¥ë“¤ì˜ í† í°(input_ids) ê°œìˆ˜ê°€ ê°™ì•„ì•¼ í•œë‹¤.\n",
    "\n",
    "ê·¸ë˜ì„œ batchì˜ shapeì„ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ ì£¼ëŠ” ê³¼ì •ì„ collateë¼ê³  í•œë‹¤.\n",
    "\n",
    "### Collate\n",
    "* list -> pytorchì˜ tensorë¡œ ë³€í™˜\n",
    "* batch size í†µì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32b9aa0-2f5a-4883-a2b9-ca15b520932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ\n",
    "data_dir = \"./data/nsmc\"  # ë°ì´í„° ì €ì¥ ê²½ë¡œ\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_file = f\"{data_dir}/ratings_train.txt\"\n",
    "test_file = f\"{data_dir}/ratings_test.txt\"\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    from Korpora import Korpora\n",
    "    Korpora.fetch(\"nsmc\", root_dir=data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce351a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… 1. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# âœ… 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (í•„ìš”í•˜ë©´ ìƒ˜í”Œë§í•˜ì—¬ í…ŒìŠ¤íŠ¸)\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\").dropna().sample(5000)  # ì¼ë¶€ë§Œ ì‚¬ìš©\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\").dropna().sample(1000)\n",
    "\n",
    "# âœ… 3. ìµœì í™”ëœ Dataset (ë¯¸ë¦¬ í† í°í™” ì ìš©)\n",
    "class NsmcDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df[\"document\"].astype(str).tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "\n",
    "        # ğŸ”¥ ë¯¸ë¦¬ í•œ ë²ˆì— í† í°í™”í•˜ì—¬ ì €ì¥\n",
    "        self.encodings = self.tokenizer(\n",
    "            self.texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# âœ… 4. ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = NsmcDataset(train_df, tokenizer)\n",
    "test_dataset = NsmcDataset(test_df, tokenizer)\n",
    "\n",
    "# âœ… 5. ìµœì í™”ëœ DataLoader ì„¤ì •\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,  # RandomSampler ì œê±°\n",
    "    drop_last=False,\n",
    "    num_workers=0,  # í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ë”°ë¼ ì¡°ì ˆ (4~6 ì¶”ì²œ)\n",
    "    pin_memory=True,  # GPU ì†ë„ í–¥ìƒ\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# âœ… 6. ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "import time\n",
    "start_time = time.time()\n",
    "sample = next(iter(train_dataloader))\n",
    "print(f\"ë¡œë”© ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ\")\n",
    "\n",
    "# âœ… ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "print(sample[\"input_ids\"].shape)  # torch.Size([32, 128])\n",
    "print(sample[\"label\"])  # tensor([0, 1, 1, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2124a-ca1b-4ca5-bf3a-84a65647421f",
   "metadata": {},
   "source": [
    "### Pytorch Lightning\n",
    "https://minjoo-happy-blog.tistory.com/140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605cf60-4950-4efa-9777-f5b7b8d55b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AdamW\n",
    "\n",
    "# PyTorch Lightningì„ ì‚¬ìš©í•œ ê°ì„± ë¶„ì„ ëª¨ë¸ ì •ì˜\n",
    "class SentimentClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •\n",
    "task = SentimentClassificationTask(model, learning_rate=args.learning_rate)\n",
    "\n",
    "# PyTorch Lightning Trainer ì„¤ì •\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,  # í•™ìŠµ íšŸìˆ˜\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders=train_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
