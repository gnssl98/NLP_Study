{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aaec9e",
   "metadata": {},
   "source": [
    "# Do it! BERT와 GPT로 배우는 자연어 처리\n",
    "### 저자 이기창"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d7731",
   "metadata": {},
   "source": [
    "# 바이트 페어 인코딩(Byte Parir Encoding : BPE)\n",
    "* 본래 정보를 압축하는 알고리즘으로 제안 -> 최근 NLP 모델에서 널리 쓰이는 토큰화 기법\n",
    "* 데이터에서 가장 많이 등장한 무자열을 병합해서 데이터를 압축하는 기법\n",
    "\n",
    "> aaabdaabac<br>\n",
    "\n",
    "<br>\n",
    "데이터에 등장한 글자(a,b,c,d)를 초기 사전으로 구성하며, 연속된 두 글자를 한 글자로 병합<br>\n",
    "aa-> Z로 병합<br>\n",
    "<br>\n",
    "\n",
    "> ZabdZabac\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "ab가 다음으로 많이 나왔으므로 ab -> Y로 병합<br>\n",
    "<br>\n",
    "\n",
    "> ZYdZYac\n",
    "\n",
    "<br>\n",
    "ZY가 다음으로 많이 나왔으므로 ZY -> X로 병합(이미 병합된 문자열도 다시 병합)<br>\n",
    "<br>\n",
    "\n",
    "> XdXac\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 사전\n",
    "(a,b,c,d) -> (a,b,c,d,Z,Y,X)<br><br>\n",
    "5개 -> 7개<br>\n",
    "len(데이터) 11 -> 5<br>\n",
    "<br>\n",
    "* BPE 기반 토큰화 기법은corpus에서 자주 등장하는 문자열(Subword)을 토큰으로 분석하기 때문에 분석 대상 언어에 대한 지식이 필요 X<br>\n",
    "<br>\n",
    "## 과정\n",
    "1. 어휘 집합 구축: 자주 등장하는 문자열 병합, 어휘 집합에 추가(원하는 어휘 집합 크기가 될 때 까지 반복)\n",
    "2. 토큰화 : 토큰화 대상 문장의 각 어절에서 어휘 집합에 있는 서브워드가 포함되었을 때 항상 서브워드를 어절에서 분리\n",
    "\n",
    "\n",
    "# BRE 어휘 집합 구축\n",
    "\n",
    "### Pre-Tokenize\n",
    "Corpus의 모든 문장을 공백으로 나누는 작업\n",
    "\n",
    "1. Pre-Tokenize을 적용하고 단어의 빈도를 모두 세어서 초기의 어휘 집합 구성\n",
    "b,g,h,n,p,s,u\n",
    "\n",
    "\n",
    "### Pre-Tokenize 결과\n",
    "\n",
    "| 토큰  | 빈도 |\n",
    "|------|------|\n",
    "| hug  | 10   |\n",
    "| pug  | 5    |\n",
    "| pun  | 12   |\n",
    "| bun  | 4    |\n",
    "| hugs | 5    |\n",
    "\n",
    "### 초기 어휘 집합으로 다시 작성한 빈도표\n",
    "\n",
    "| 토큰        | 빈도 |\n",
    "|------------|------|\n",
    "| h, u, g   | 10   |\n",
    "| p, u, g   | 5    |\n",
    "| p, u, n   | 12   |\n",
    "| b, u, n   | 4    |\n",
    "| h, u, g, s | 5    |\n",
    "\n",
    "다시 두쌍으로 묶어서 bigram 생성 및 병합\n",
    "\n",
    "| 바이그램 쌍 | 빈도 |\n",
    "|------------|------|\n",
    "| b, u      | 4    |\n",
    "| g, s      | 5    |\n",
    "| h, u      | 15   |\n",
    "| p, u      | 17   |\n",
    "| u, g      | 20   |\n",
    "| u, n      | 16   |\n",
    "\n",
    "u,g 가 20개로 제일 많아서 병합(u,g -> ug)\n",
    "\n",
    "> b,g,h,n,p,s,u,ug\n",
    "\n",
    "| 바이그램 쌍 | 빈도 |\n",
    "|------------|------|\n",
    "| b, u      | 4    |\n",
    "| h, ug      | 15    |\n",
    "| p, u      | 12   |\n",
    "| p, ug      | 5   |\n",
    "| u, n      | 16   |\n",
    "| ug, s     | 5  |\n",
    "\n",
    "u,n 가 16개로 제일 많아서 병합 (u,n -> un)\n",
    "\n",
    "> b,g,h,n,p,s,u,ug,un\n",
    "\n",
    "| 바이그램 쌍 | 빈도 |\n",
    "|------------|------|\n",
    "| b, un     | 4    |\n",
    "| h, ug     | 15   |\n",
    "| p, ug     | 5    |\n",
    "| p, un     | 12   |\n",
    "| ug, s     | 5    |\n",
    "\n",
    "## BPE 어휘 집합 구축\n",
    "\n",
    "> b, g, h, n, p, s, u, ug, un, hug\n",
    "\n",
    "## BPE 어휘 집합은 고빈도 바이그램 쌍을 병합하는 방식으로 구축\n",
    "\n",
    "처음 병합한 대상은 u, g, 두 번째는 u, n, 마지막은 h, ug였음을 확인할 수 있다.<br>\n",
    "이 내용 그대로 merges.txt로 저장<br>\n",
    "병합 우선순위(merges.txt)<br>\n",
    "u g<br>\n",
    "u n<br>\n",
    "h ug<br>\n",
    "\n",
    "\n",
    "# BPE 토큰화\n",
    "\n",
    "어휘 집합(vocab.json), 병합 우선순위(merge.txt)가 있으면 토큰화 수행 가능\n",
    "<br>\n",
    "ex) pug bug mug 라는 문장을 토큰화\n",
    "\n",
    "> pug bug mug -> pug,bug,mug\n",
    "\n",
    "1. 문자 단위로 분리\n",
    "\n",
    "> pug -> p,u,g\n",
    "\n",
    "2. merge.txt 파일을 참고해 병합 우선순위를 부여\n",
    "\n",
    "> p,u -> 우선 순위 없음\n",
    "<br> u,g -> 1순위\n",
    "\n",
    "3. u,g의 우선 순위가 높으므로 먼저 합친다.\n",
    "\n",
    "> p,u,g -> p, ug\n",
    "\n",
    "4. merge.txt 파일을 한번 더 참고해 병합 우선수위를 부여\n",
    "\n",
    "> p,ug -> 우선 순위 없음\n",
    "\n",
    "병합할 대상이 없어서 stop <br>\n",
    "이 순서를 반복<br>\n",
    "&lt;unk&gt; -> 미등록 토큰<br>\n",
    "여기서 m이 어휘 집합에 없어서 미등록 토큰이 된다.\n",
    "\n",
    "> pug bug mug -> p, ug, b, ug, &lt;unk&gt;, ug\n",
    "\n",
    "# WordPiece\n",
    "\n",
    "* Corpus에서 자주 등장한 문자열을 토큰으로 인식 -> BPE와 본질적으로 유사\n",
    "* 어휘 집합을 구축할 때 문자열을 병합하는 기준이 다름\n",
    "* 빈도를 기준으로 병합 x -> 병합했을 때 Corpus의 Likelihood를 가장 높이는 쌍을 병합\n",
    "\n",
    "<br>\n",
    "병합 후보가 a,b 일때, #a, #b, #ab는 각각 a,b,ab라는 문자열의 빈도수, n은 전체 글자 수를 가리킨다.\n",
    "<br>\n",
    "분자는 ab가 연이어 등장할 확률, 분모는 a,b가 각각 등장할 확률의 곱\n",
    "<br>\n",
    "\n",
    "$\\frac{\\left(\\frac{\\# ab}{n} \\right)}{\\left(\\frac{\\# a}{n} \\right)} \\times \\left(\\frac{\\# b}{n} \\right)$\n",
    "\n",
    "이 수식의 값이 커지려면 a와 b가 서로 독립임을 가정했을 때보다 둘이 자주 동시에 등장해야 한다.<br>\n",
    "워드피스에서는 병합 후보에 오른 쌍을 미리 병합해 보고 잃는 것과 가치 등을 판단한 후에 병합. (병합 대상 전체 후보들 가운데 위와 같이 계산한 값이 가장 높은 쌍을 합친다.)\n",
    "<br>\n",
    "\n",
    "워드피스는 어휘 집합(vocab.txt)만 가지고 토큰화\n",
    "<br>\n",
    "워드피스에서는 분석 대상 어절에 어휘 집합에 있는 서브워드가 포홤돼 있을 때 해당 서브워드를 어절에서 분리<br>\n",
    "단, 이러한 서브워드 후보가 여럿 있을 경우 가장 긴 서브워드를 선택.\n",
    "<br>\n",
    "이후 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고(최장 일치 기준), 또 분리\n",
    "<br>\n",
    "분석 대상 문자열에서 서브워드 후보가 하나도 없으면 해당 문자열 전체를 미등록 단어로 취급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5d2f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 11.0MB/s]                     \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 9.88MB/s]                      \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "nsmc = Korpora.load(\"nsmc\", force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd183cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def write_lines(path, lines):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(f'{line}\\n')\n",
    "write_lines(\"./data/tokenizer/train.txt\", nsmc.train.get_all_texts())\n",
    "write_lines(\"./data/tokenizer/test.txt\", nsmc.test.get_all_texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfe07d3-2bd4-41e7-8a72-d512aeb4abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./bbpe\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10c693-e322-4776-84b3-c941c661af10",
   "metadata": {},
   "source": [
    "vocab.json과 merges.txt가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80948ecd-162a-45fa-9362-76e4a8f4439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./bbpe/vocab.json', './bbpe/merges.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "bytebpe_tokenizer.train(\n",
    "    files=[\"./data/tokenizer/train.txt\",\"./data/tokenizer/test.txt\"],# corpus를 리스트 형태로 넣기\n",
    "          vocab_size=10000, # 어휘 집합 크기 조절\n",
    "          special_tokens=[\"[PAD]\"] #특수 토큰 추가\n",
    ")\n",
    "bytebpe_tokenizer.save_model(\"./bbpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6b6ca1-367f-42cc-aeb7-e718d78babb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./wordpiece/vocab.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "wordpiece_tokenizer.train(\n",
    "    files=[\"./data/tokenizer/train.txt\",\"./data/tokenizer/test.txt\"],\n",
    "    vocab_size=10000,\n",
    ")\n",
    "wordpiece_tokenizer.save_model(\"./wordpiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd90003b-e881-4a45-9071-4e99ff2dcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT 토크나이저 선언\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"./bbpe\")\n",
    "tokenizer_gpt.pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e325feab-8539-49f2-b117-89bf13619945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT 토크나이저로 토큰화하기\n",
    "sentences = [\n",
    "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
    "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
    "    \"별루 였다..\",\n",
    "]\n",
    "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2016f1-3179-41ca-ace9-01f2078c0d04",
   "metadata": {},
   "source": [
    "## tokenized_sentences\n",
    "* GPT2 토크나이저(tokenizer_gpt)를 사용하여 개별 문장을 토큰화\n",
    "* .tokenize(sentence): 해당 문장을 개별 토큰 단위로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4368b283-34c1-4c22-9c16-ce3650a21149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
       " ['íĿł',\n",
       "  '...',\n",
       "  'íı¬ìĬ¤íĦ°',\n",
       "  'ë³´ê³ł',\n",
       "  'Ġì´ĪëĶ©',\n",
       "  'ìĺģíĻĶ',\n",
       "  'ì¤Ħ',\n",
       "  '....',\n",
       "  'ìĺ¤ë²Ħ',\n",
       "  'ìĹ°ê¸°',\n",
       "  'ì¡°ì°¨',\n",
       "  'Ġê°Ģë³į',\n",
       "  'ì§Ģ',\n",
       "  'ĠìķĬ',\n",
       "  'êµ¬ëĤĺ'],\n",
       " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f5b49fc-de78-46ba-9c23-4167073e80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT 모델 입력 만들기\n",
    "batch_inputs = tokenizer_gpt(\n",
    "    sentences,\n",
    "    padding=\"max_length\",  # 문장의 최대 길이에 맞춰 패딩\n",
    "    max_length=12,  # 문장의 토큰 기준 최대 길이\n",
    "    truncation=True,  # 문장 잘림 허용 옵션\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2e167-5c91-4d12-8345-53db7c53d118",
   "metadata": {},
   "source": [
    "## batch_inputs\n",
    "모델 입력을 위한 인코딩\n",
    "* .tokenize()와 다르게 토큰화 + 숫자 변환(input_ids) + 패딩(attention_mask) 적용됨\n",
    "* max_length=12: 최대 12개 토큰까지 유지 (이보다 길면 자름)\n",
    "* padding=\"max_length\": 부족한 길이는 [PAD] 토큰으로 패딩 처리\n",
    "* truncation=True: 최대 길이를 넘으면 자동으로 잘림\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15109eb4-6465-4962-b116-5392666a6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0], [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1162, 16, 3], [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e4208d3-48d3-43f6-af7e-f5a93b6f4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT 토크나이저 선언\n",
    "from transformers import BertTokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\n",
    "    \"./wordpiece\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39fb816e-0b24-4d2e-802f-377d772ee8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT 토크나이저로 토큰화하기\n",
    "sentences = [\n",
    "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
    "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
    "    \"별루 였다..\",\n",
    "]\n",
    "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d6debcc-058f-4a2b-999c-f76bb88fbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT 모델 입력 만들기\n",
    "batch_inputs = tokenizer_bert(\n",
    "    sentences,\n",
    "    padding=\"max_length\",\n",
    "    max_length=12,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9866c2e6-1d10-497a-aaef-982ff28c7875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리'],\n",
       " ['흠',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '포스터',\n",
       "  '##보고',\n",
       "  '초딩',\n",
       "  '##영화',\n",
       "  '##줄',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '오버',\n",
       "  '##연기',\n",
       "  '##조차',\n",
       "  '가볍',\n",
       "  '##지',\n",
       "  '않',\n",
       "  '##구나'],\n",
       " ['별루', '였다', '.', '.']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784d0f5-c46e-4d1d-a823-2dd85fff30db",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### OpenAI의 최신 GPT 모델(GPT-3, GPT-3.5, GPT-4)은 tiktoken 라이브러리에서 제공하는 cl100k_base 토크나이저를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c090016-0108-4365-b7dc-070e78a2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdd8ea79-3bef-4329-8bd5-feb92f1fba49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences: [[54059, 5251, 235, 242, 167, 117, 247, 497, 49011, 226, 17164, 250, 49011, 250, 96064, 251, 61415, 76242, 97, 36811, 38078, 102, 44690, 29102], [169, 251, 254, 1131, 169, 237, 105, 25941, 34961, 42771, 35495, 84415, 67598, 102, 36092, 223, 57390, 59269, 226, 1975, 58368, 80104, 13879, 108, 21121, 93917, 89641, 101, 36609, 29099, 235, 22035, 51796, 89359, 61415], [29099, 226, 53987, 101, 39623, 222, 13447, 497]]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "sentences = [\n",
    "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
    "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
    "    \"별루 였다..\",\n",
    "]\n",
    "\n",
    "tokenized_sentences = [tokenizer_gpt4.encode(sentence) for sentence in sentences]\n",
    "\n",
    "print(\"Tokenized Sentences:\", tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81743659-54e8-4c3f-ae9d-13758ff502e1",
   "metadata": {},
   "source": [
    "# cl100k_base\n",
    "* Byte-Pair Encoding (BPE) + 최적화된 서브워드(subword) 단위 분할 방식을 사용하여 작동\n",
    "1.\t이전 모델(GPT-3)보다 압축된 토큰화 구조<br>\n",
    "→ 동일한 문장에서도 더 적은 수의 토큰을 생성 (비용 절감 효과)\n",
    "2.\t자주 등장하는 단어를 하나의 토큰으로 처리<br>\n",
    "→ 영어의 경우 “Hello”는 [Hello] (1토큰), “running”은 [run, ning] (2토큰)<br>\n",
    "→ 한국어의 경우 “안녕하세요”는 [안녕하세요] (1토큰)<br>\n",
    "3. 이전 토크나이저(p50k_base, r50k_base)보다 더 효율적<br>\n",
    "→ 같은 문장을 적은 수의 토큰으로 변환<br>\n",
    "4.\tGPT-4, GPT-3.5 (gpt-3.5-turbo, gpt-4)에 최적화됨<br>\n",
    "\n",
    "### 개선점\n",
    "1. 더 짧은 토큰 길이 → GPT-3 대비 10~20% 적은 토큰 수 사용\n",
    "2. 한국어, 일본어, 중국어 최적화 → 기존 GPT-3보다 성능 향상\n",
    "3. 효율적인 패딩 & 토큰 구조 → 불필요한 단어 분할 감소\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbf32823-d375-48f2-9927-8aade5df8d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 토큰 ID: [31495, 230, 75265, 243, 92245, 0, 74177, 15478, 246, 38295, 254, 168, 242, 101, 20565, 66799, 233, 76242, 97, 36811, 13]\n",
      "📌 디코딩된 텍스트: 안녕하세요! 오늘 날씨가 좋네요.\n"
     ]
    }
   ],
   "source": [
    "sentence_ko = \"안녕하세요! 오늘 날씨가 좋네요.\"\n",
    "tokenized_ko = tokenizer_gpt4.encode(sentence_ko)\n",
    "\n",
    "print(\"📌 토큰 ID:\", tokenized_ko)\n",
    "print(\"📌 디코딩된 텍스트:\", tokenizer_gpt4.decode(tokenized_ko))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ac07f-bd43-4487-92cf-5992497e7597",
   "metadata": {},
   "source": [
    "* “안녕하세요” → [13392] (1개의 토큰으로 처리됨)\n",
    "* “날씨가” → [22035] (BPE를 통해 하나의 토큰으로 압축됨)\n",
    "* “좋네요” → [17606] (한 덩어리로 처리됨, 불필요한 분할 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b61e35-cf3b-4832-91c4-c9ab41366186",
   "metadata": {},
   "source": [
    "참고 논문\n",
    "*  Neural Machine Translation of Rare Words with Subword Units <br>\n",
    "https://arxiv.org/pdf/1508.07909\n",
    " * Japanese and Korean Voice Search<br>\n",
    " https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384fd33-33ea-4732-9e9f-94c23e0555fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
