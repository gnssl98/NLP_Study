{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aaec9e",
   "metadata": {},
   "source": [
    "# Do it! BERT와 GPT로 배우는 자연어 처리\n",
    "### 저자 이기창"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d7731",
   "metadata": {},
   "source": [
    "# 바이트 페어 인코딩(Byte Parir Encoding : BPE)\n",
    "* 본래 정보를 압축하는 알고리즘으로 제안 -> 최근 NLP 모델에서 널리 쓰이는 토큰화 기법\n",
    "* 데이터에서 가장 많이 등장한 무자열을 병합해서 데이터를 압축하는 기법\n",
    "\n",
    "> aaabdaabac<br>\n",
    "\n",
    "<br>\n",
    "데이터에 등장한 글자(a,b,c,d)를 초기 사전으로 구성하며, 연속된 두 글자를 한 글자로 병합<br>\n",
    "aa-> Z로 병합<br>\n",
    "<br>\n",
    "\n",
    "> ZabdZabac\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "ab가 다음으로 많이 나왔으므로 ab -> Y로 병합<br>\n",
    "<br>\n",
    "\n",
    "> ZYdZYac\n",
    "\n",
    "<br>\n",
    "ZY가 다음으로 많이 나왔으므로 ZY -> X로 병합(이미 병합된 문자열도 다시 병합)<br>\n",
    "<br>\n",
    "\n",
    "> XdXac\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 사전\n",
    "(a,b,c,d) -> (a,b,c,d,Z,Y,X)<br><br>\n",
    "5개 -> 7개<br>\n",
    "len(데이터) 11 -> 5<br>\n",
    "<br>\n",
    "* BPE 기반 토큰화 기법은corpus에서 자주 등장하는 문자열(Subword)을 토큰으로 분석하기 때문에 분석 대상 언어에 대한 지식이 필요 X<br>\n",
    "<br>\n",
    "## 과정\n",
    "1. 어휘 집합 구축: 자주 등장하는 문자열 병합, 어휘 집합에 추가(원하는 어휘 집합 크기가 될 때 까지 반복)\n",
    "2. 토큰화 : 토큰화 대상 문장의 각 어절에서 어휘 집합에 있는 서브워드가 포함되었을 때 항상 서브워드를 어절에서 분리\n",
    "\n",
    "\n",
    "# BRE 어휘 집합 구축\n",
    "\n",
    "### Pre-Tokenize\n",
    "Corpus의 모든 문장을 공백으로 나누는 작업\n",
    "\n",
    "1. Pre-Tokenize을 적용하고 단어의 빈도를 모두 세어서 초기의 어휘 집합 구성\n",
    "b,g,h,n,p,s,u\n",
    "\n",
    "\n",
    "### Pre-Tokenize 결과\n",
    "\n",
    "| 토큰  | 빈도 |\n",
    "|------|------|\n",
    "| hug  | 10   |\n",
    "| pug  | 5    |\n",
    "| pun  | 12   |\n",
    "| bun  | 4    |\n",
    "| hugs | 5    |\n",
    "\n",
    "### 초기 어휘 집합으로 다시 작성한 빈도표\n",
    "\n",
    "| 토큰        | 빈도 |\n",
    "|------------|------|\n",
    "| h, u, g   | 10   |\n",
    "| p, u, g   | 5    |\n",
    "| p, u, n   | 12   |\n",
    "| b, u, n   | 4    |\n",
    "| h, u, g, s | 5    |\n",
    "\n",
    "다시 두쌍으로 묶어서 bigram 생성 및 병합\n",
    "\n",
    "| 바이그램 쌍 | 빈도 |\n",
    "|------------|------|\n",
    "| b, u      | 4    |\n",
    "| g, s      | 5    |\n",
    "| h, u      | 15   |\n",
    "| p, u      | 17   |\n",
    "| u, g      | 20   |\n",
    "| u, n      | 16   |\n",
    "\n",
    "u,g 가 20개로 제일 많아서 병합(u,g -> ug)\n",
    "\n",
    "> b,g,h,n,p,s,u,ug\n",
    "\n",
    "| 바이그램 쌍 | 빈도 |\n",
    "|------------|------|\n",
    "| b, u      | 4    |\n",
    "| h, ug      | 15    |\n",
    "| p, u      | 12   |\n",
    "| p, ug      | 5   |\n",
    "| u, n      | 16   |\n",
    "| ug, s     | 5  |\n",
    "\n",
    "u,n 가 16개로 제일 많아서 병합 (u,n -> un)\n",
    "\n",
    "> b,g,h,n,p,s,u,ug,un\n",
    "\n",
    "| 바이그램 쌍 | 빈도 |\n",
    "|------------|------|\n",
    "| b, un     | 4    |\n",
    "| h, ug     | 15   |\n",
    "| p, ug     | 5    |\n",
    "| p, un     | 12   |\n",
    "| ug, s     | 5    |\n",
    "\n",
    "## BPE 어휘 집합 구축\n",
    "\n",
    "> b, g, h, n, p, s, u, ug, un, hug\n",
    "\n",
    "## BPE 어휘 집합은 고빈도 바이그램 쌍을 병합하는 방식으로 구축\n",
    "\n",
    "처음 병합한 대상은 u, g, 두 번째는 u, n, 마지막은 h, ug였음을 확인할 수 있다.<br>\n",
    "이 내용 그대로 merges.txt로 저장<br>\n",
    "병합 우선순위(merges.txt)<br>\n",
    "u g<br>\n",
    "u n<br>\n",
    "h ug<br>\n",
    "\n",
    "\n",
    "# BPE 토큰화\n",
    "\n",
    "어휘 집합(vocab.json), 병합 우선순위(merge.txt)가 있으면 토큰화 수행 가능\n",
    "<br>\n",
    "ex) pug bug mug 라는 문장을 토큰화\n",
    "\n",
    "> pug bug mug -> pug,bug,mug\n",
    "\n",
    "1. 문자 단위로 분리\n",
    "\n",
    "> pug -> p,u,g\n",
    "\n",
    "2. merge.txt 파일을 참고해 병합 우선순위를 부여\n",
    "\n",
    "> p,u -> 우선 순위 없음\n",
    "<br> u,g -> 1순위\n",
    "\n",
    "3. u,g의 우선 순위가 높으므로 먼저 합친다.\n",
    "\n",
    "> p,u,g -> p, ug\n",
    "\n",
    "4. merge.txt 파일을 한번 더 참고해 병합 우선수위를 부여\n",
    "\n",
    "> p,ug -> 우선 순위 없음\n",
    "\n",
    "병합할 대상이 없어서 stop <br>\n",
    "이 순서를 반복<br>\n",
    "&lt;unk&gt; -> 미등록 토큰<br>\n",
    "여기서 m이 어휘 집합에 없어서 미등록 토큰이 된다.\n",
    "\n",
    "> pug bug mug -> p, ug, b, ug, &lt;unk&gt;, ug\n",
    "\n",
    "# WordPiece\n",
    "\n",
    "* Corpus에서 자주 등장한 문자열을 토큰으로 인식 -> BPE와 본질적으로 유사\n",
    "* 어휘 집합을 구축할 때 문자열을 병합하는 기준이 다름\n",
    "* 빈도를 기준으로 병합 x -> 병합했을 때 Corpus의 Likelihood를 가장 높이는 쌍을 병합\n",
    "\n",
    "<br>\n",
    "병합 후보가 a,b 일때, #a, #b, #ab는 각각 a,b,ab라는 문자열의 빈도수, n은 전체 글자 수를 가리킨다.\n",
    "<br>\n",
    "분자는 ab가 연이어 등장할 확률, 분모는 a,b가 각각 등장할 확률의 곱\n",
    "<br>\n",
    "\n",
    "$\\frac{\\left(\\frac{\\# ab}{n} \\right)}{\\left(\\frac{\\# a}{n} \\right)} \\times \\left(\\frac{\\# b}{n} \\right)$\n",
    "\n",
    "이 수식의 값이 커지려면 a와 b가 서로 독립임을 가정했을 때보다 둘이 자주 동시에 등장해야 한다.<br>\n",
    "워드피스에서는 병합 후보에 오른 쌍을 미리 병합해 보고 잃는 것과 가치 등을 판단한 후에 병합. (병합 대상 전체 후보들 가운데 위와 같이 계산한 값이 가장 높은 쌍을 합친다.)\n",
    "<br>\n",
    "\n",
    "워드피스는 어휘 집합(vocab.txt)만 가지고 토큰화\n",
    "<br>\n",
    "워드피스에서는 분석 대상 어절에 어휘 집합에 있는 서브워드가 포홤돼 있을 때 해당 서브워드를 어절에서 분리<br>\n",
    "단, 이러한 서브워드 후보가 여럿 있을 경우 가장 긴 서브워드를 선택.\n",
    "<br>\n",
    "이후 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고(최장 일치 기준), 또 분리\n",
    "<br>\n",
    "분석 대상 문자열에서 서브워드 후보가 하나도 없으면 해당 문자열 전체를 미등록 단어로 취급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5d2f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 43.3MB/s]                                                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 30.7MB/s]                                                             \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "nsmc = Korpora.load(\"nsmc\", force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd183cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (<myvenv>)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
