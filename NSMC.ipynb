{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f602cf3-d1f4-4760-a5b4-1fd818d619b5",
   "metadata": {},
   "source": [
    "## NSMC(Naver Sentiment Movie Corpis\n",
    "ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë§ë­‰ì¹˜<br>\n",
    "from Korpora import Korpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae3e3b-b4a8-48b4-89e7-009c4518d74f",
   "metadata": {},
   "source": [
    " * ì‚¬ìš©í•˜ëŠ” ëª¨ë¸: beomi/kcbert-base\n",
    " *  ëª¨ë¸ êµ¬ì¡°: BertForSequenceClassification (BERT ëª¨ë¸ì„ ê°ì„± ë¶„ì„ìš© ë¶„ë¥˜ê¸°ë¡œ ë³€í™˜)\n",
    " * ì‚¬ì „ í•™ìŠµëœ ë°ì´í„°: í•œêµ­ì–´ ë°ì´í„°(Korean)ë¡œ ì‚¬ì „ í•™ìŠµëœ KcBERT ëª¨ë¸\n",
    " * ì¶”ê°€ í•™ìŠµí•˜ëŠ” ë°ì´í„°: NSMC(ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„) ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df41cf5-f535-42ce-9704-b8e94237ff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0\n",
      "MPS ì§€ì› ì—¬ë¶€: True\n",
      "MPS ì‚¬ìš© ê°€ëŠ¥: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"MPS ì§€ì› ì—¬ë¶€:\", torch.backends.mps.is_available())  # Trueì—¬ì•¼ ì •ìƒ ì‘ë™\n",
    "print(\"MPS ì‚¬ìš© ê°€ëŠ¥:\", torch.backends.mps.is_built())  # Trueì—¬ì•¼ ì •ìƒ ì‘ë™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39235ec-8fa9-4cc6-b753-a7900c6770ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7b7f4-1d19-4c32-a8fd-faf7d9b07856",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "í† í°í™” ìˆ˜í–‰ í”„ë¡œê·¸ë¨<br>\n",
    "kcbert-base ëª¨ë¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f43528-f66b-45cd-8450-29785e6f840b",
   "metadata": {},
   "source": [
    "# Pytorch's Data Loader\n",
    "\n",
    "* íŒŒì´í† ì¹˜ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ë ¤ë©´ ë°˜ë“œì‹œ ì •ì˜í•´ì•¼ í•œë‹¤.\n",
    "* ë°ì´í„°ë¥¼ ë°°ì¹˜(batch)ë‹¨ìœ„ë¡œ ëª¨ë¸ì— ë°€ì–´ ë„£ì–´ì£¼ëŠ” ì—­í• \n",
    "* ì „ì²´ ë°ì´í„° ê°€ìš´ë° ì¼ë¶€ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë½‘ì•„ ë°°ì¹˜ë¥¼ êµ¬ì„±\n",
    "* ë°ì´í„°ì…‹ì€ ë°ì´í„° ë¡œë”ì˜ êµ¬ì„± ìš”ì†Œ ì¤‘ í•˜ë‚˜\n",
    "* ë°ì´í„°ì…‹ì€ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë³´ìœ \n",
    "\n",
    "ë°ì´í„° ë¡œë” > ë°ì´í„°ì…‹ > ì¸ìŠ¤í„´ìŠ¤\n",
    "\n",
    "* batchëŠ” ê·¸ ëª¨ì–‘ì´ ê³ ì •ì ì´ì–´ì•¼ í•  ë•Œê°€ ë§ë‹¤. -> ë¬¸ì¥ë“¤ì˜ í† í°(input_ids) ê°œìˆ˜ê°€ ê°™ì•„ì•¼ í•œë‹¤.\n",
    "\n",
    "ê·¸ë˜ì„œ batchì˜ shapeì„ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ ì£¼ëŠ” ê³¼ì •ì„ collateë¼ê³  í•œë‹¤.\n",
    "\n",
    "### Collate\n",
    "* list -> pytorchì˜ tensorë¡œ ë³€í™˜\n",
    "* batch size í†µì¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2124a-ca1b-4ca5-bf3a-84a65647421f",
   "metadata": {},
   "source": [
    "### Pytorch Lightning\n",
    "https://minjoo-happy-blog.tistory.com/140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b15a9bb-9f11-4e58-8021-404bdd86ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” KcBERT ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\n",
      "ğŸ“Š ì •í™•ë„ (Accuracy): 49.43%\n",
      "ğŸ“Š ìƒì„¸ í‰ê°€ ê²°ê³¼:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ë¶€ì • ğŸ˜¡       0.49      0.59      0.54     24826\n",
      "        ê¸ì • ğŸ˜Š       0.50      0.40      0.44     25171\n",
      "\n",
      "    accuracy                           0.49     49997\n",
      "   macro avg       0.49      0.49      0.49     49997\n",
      "weighted avg       0.49      0.49      0.49     49997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from Korpora import Korpora\n",
    "\n",
    "# KcBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "MODEL_NAME = \"beomi/kcbert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# GPU(MPS) ë˜ëŠ” CPU ì„¤ì •\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # í‰ê°€ ëª¨ë“œ\n",
    "\n",
    "# NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ\n",
    "data_dir = \"./data/nsmc\"\n",
    "test_file = f\"{data_dir}/ratings_test.txt\"\n",
    "\n",
    "# ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ ë‹¤ìš´ë¡œë“œ\n",
    "if not os.path.exists(test_file):\n",
    "    Korpora.fetch(\"nsmc\", root_dir=data_dir)\n",
    "\n",
    "# NSMC í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\").dropna()\n",
    "\n",
    "# NSMC ë°ì´í„°ì…‹ì„ PyTorch Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "class NsmcDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            df[\"document\"].astype(str).tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(df[\"label\"].tolist(), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\n",
    "test_dataset = NsmcDataset(test_df, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ì¶œë ¥\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"ğŸ“Š ì •í™•ë„ (Accuracy): {acc * 100:.2f}%\")\n",
    "    print(\"ğŸ“Š ìƒì„¸ í‰ê°€ ê²°ê³¼:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"ë¶€ì • ğŸ˜¡\", \"ê¸ì • ğŸ˜Š\"]))\n",
    "\n",
    "# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰\n",
    "print(\"ğŸ” KcBERT ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7fd51d-b7e8-49a4-b20c-d6eca1e38012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ ì¤‘...\n",
      "ëª¨ë¸ ì •í™•ë„: 51.10%\n",
      "ì…ë ¥ ë¬¸ì¥: ì´ ì˜í™”ëŠ” ì •ë§ ìµœê³ ì˜€ì–´!\n",
      "ì˜ˆì¸¡ ê²°ê³¼: ë¶€ì • ğŸ˜¡ (í™•ë¥ : 51.04%)\n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì™„ì „ ìµœì•…ì´ì•¼, ì‹œê°„ ë‚­ë¹„í–ˆì–´.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: ë¶€ì • ğŸ˜¡ (í™•ë¥ : 52.02%)\n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ê·¸ëƒ¥ ê·¸ë¬ì–´. ë³„ë¡œ ê°í¥ì´ ì—†ì—ˆì–´.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: ë¶€ì • ğŸ˜¡ (í™•ë¥ : 57.55%)\n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ë°°ìš° ì—°ê¸°ê°€ ë„ˆë¬´ í›Œë¥­í–ˆì–´!\n",
      "ì˜ˆì¸¡ ê²°ê³¼: ë¶€ì • ğŸ˜¡ (í™•ë¥ : 52.94%)\n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ì§€ë£¨í–ˆì–´.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: ë¶€ì • ğŸ˜¡ (í™•ë¥ : 52.74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass  \n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "@dataclass\n",
    "class ClassificationTrainArguments:\n",
    "    pretrained_model_name: str\n",
    "    downstream_corpus_name: str\n",
    "    downstream_corpus_root_dir: str\n",
    "    downstream_model_dir: str\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",  # KC-BERT ì‚¬ìš©\n",
    "    downstream_corpus_name=\"nsmc\",\n",
    "    downstream_corpus_root_dir=\"./data\",\n",
    "    downstream_model_dir=\"./model\",\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# MPS(GPU) ìë™ ê°ì§€ ë° ì„¤ì •\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "data_dir = f\"{args.downstream_corpus_root_dir}/{args.downstream_corpus_name}\"\n",
    "train_file = f\"{data_dir}/ratings_train.txt\"\n",
    "test_file = f\"{data_dir}/ratings_test.txt\"\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    Korpora.fetch(args.downstream_corpus_name, root_dir=data_dir)\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ìˆœì„œ ìˆ˜ì •)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.pretrained_model_name, do_lower_case=False)\n",
    "\n",
    "pretrained_model_config = BertConfig.from_pretrained(args.pretrained_model_name, num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained(args.pretrained_model_name, config=pretrained_model_config)\n",
    "\n",
    "# ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "model.to(device)\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ìƒ˜í”Œë§ ì ìš©)\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\").dropna().sample(5000)  # ì¼ë¶€ ìƒ˜í”Œ ì‚¬ìš©\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\").dropna().sample(1000)\n",
    "\n",
    "# NSMC ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "class NsmcDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            df[\"document\"].astype(str).tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(df[\"label\"].tolist(), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\n",
    "train_dataset = NsmcDataset(train_df, tokenizer)\n",
    "test_dataset = NsmcDataset(test_df, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ê°ì„± ë¶„ì„ ëª¨ë¸ ì •ì˜ (PyTorch Lightning)\n",
    "class SentimentClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "task = SentimentClassificationTask(model)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  \n",
    "    accelerator=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[pl.callbacks.EarlyStopping(monitor=\"train_loss\", patience=3)]\n",
    ")\n",
    "# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"ëª¨ë¸ ì •í™•ë„: {acc * 100:.2f}%\")\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ ì‹¤í–‰\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ ì¤‘...\")\n",
    "evaluate_model(task.model, test_dataloader)\n",
    "\n",
    "# ì˜ˆì œ ë¬¸ì¥ ê°ì • ë¶„ì„ í•¨ìˆ˜\n",
    "def predict_sentiment(model, text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "    label_map = {0: \"ë¶€ì • ğŸ˜¡\", 1: \"ê¸ì • ğŸ˜Š\"}\n",
    "    print(f\"ì…ë ¥ ë¬¸ì¥: {text}\")\n",
    "    print(f\"ì˜ˆì¸¡ ê²°ê³¼: {label_map[pred_class]} (í™•ë¥ : {probs[0][pred_class] * 100:.2f}%)\\n\")\n",
    "\n",
    "# ì˜ˆì œ ë¬¸ì¥ ê°ì • ë¶„ì„ ì‹¤í–‰\n",
    "sample_texts = [\n",
    "    \"ì´ ì˜í™”ëŠ” ì •ë§ ìµœê³ ì˜€ì–´!\",\n",
    "    \"ì™„ì „ ìµœì•…ì´ì•¼, ì‹œê°„ ë‚­ë¹„í–ˆì–´.\",\n",
    "    \"ê·¸ëƒ¥ ê·¸ë¬ì–´. ë³„ë¡œ ê°í¥ì´ ì—†ì—ˆì–´.\",\n",
    "    \"ë°°ìš° ì—°ê¸°ê°€ ë„ˆë¬´ í›Œë¥­í–ˆì–´!\",\n",
    "    \"ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ì§€ë£¨í–ˆì–´.\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    predict_sentiment(task.model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fc823-dec6-403b-a3f8-a930a6287f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
